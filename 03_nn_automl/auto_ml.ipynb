{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a6c22e1-4373-41b9-829f-8629c8bf178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b4ac36-26d8-4678-bad9-fd4825a9c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9ba3f7-60bf-4081-aace-abbed5e04761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0652a0ba-8820-429d-a559-8e805b380010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b67ec-8e4d-449d-947f-9e04226f4477",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911cb9e1-cf4c-40db-a9d9-a1bff707d0aa",
   "metadata": {},
   "source": [
    "在此次竞赛的数据集中，每个样本都对应一个葡萄牙大学的学生。原始数据集中共有4424名学生，Kaggle基于原始数据集生成了超过100,000个合成样本。对于每个学生，我们获得了人口统计数据、宏观经济数据以及课程前两个学期的表现。竞赛的目标是预测学生在三年或四年学习后的状态：是否毕业、仍在就读，或退学。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7097ab-faf9-4c32-a4ee-0b3321feddb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76518, 51012)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/train.csv', index_col=0)\n",
    "test_df = pd.read_csv('../input/test.csv', index_col=0)\n",
    "\n",
    "train_df.shape[0], test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ca1f0e-65a0-4e11-a8ef-651c22b06bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_all_feats = [col for col in train_df.columns if col != 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472aa332-eb17-4bbd-9af7-9c99c54a4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_cat_feats = [\n",
    "    'Marital status',\n",
    "    'Application mode',\n",
    "    'Application order',\n",
    "    'Course',\n",
    "    'Daytime/evening attendance',\n",
    "    'Previous qualification',\n",
    "    'Nacionality',\n",
    "    'Mother\\'s qualification',\n",
    "    'Father\\'s qualification',\n",
    "    'Mother\\'s occupation',\n",
    "    'Father\\'s occupation',\n",
    "    'Displaced',\n",
    "    'Educational special needs',\n",
    "    'Debtor',\n",
    "    'Tuition fees up to date',\n",
    "    'Gender',\n",
    "    'Scholarship holder',\n",
    "    'International',\n",
    "]\n",
    "\n",
    "ori_num_feats = [\n",
    "    'Previous qualification (grade)',\n",
    "    'Admission grade',\n",
    "    'Age at enrollment',\n",
    "    'Curricular units 1st sem (credited)',\n",
    "    'Curricular units 1st sem (enrolled)',\n",
    "    'Curricular units 1st sem (evaluations)',\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 1st sem (without evaluations)',\n",
    "    'Curricular units 2nd sem (credited)',\n",
    "    'Curricular units 2nd sem (enrolled)',\n",
    "    'Curricular units 2nd sem (evaluations)',\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)',\n",
    "    'Curricular units 2nd sem (without evaluations)',\n",
    "    'Unemployment rate',\n",
    "    'Inflation rate',\n",
    "    'GDP',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6df329db-b8c6-4ddd-bc50-4ac82ba9abc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ori_all_feats) == ( set(ori_cat_feats) | set(ori_num_feats) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07653241-8733-43b6-883f-522dcfc37591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 18, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ori_all_feats), len(ori_cat_feats), len(ori_num_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809abba5-bc05-474c-92e5-69664bb79453",
   "metadata": {},
   "source": [
    "## FE(简单)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca09bb9-4c70-42dd-b157-4286bb6c9632",
   "metadata": {},
   "source": [
    "- Tree-based 模型不需要对特征进行诸如StandardScaler MinMaxScaler之类的缩放\n",
    "- 类别特征需要做label-encode / one-hot-encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5907884-f9de-4ae1-b411-5177807d57e6",
   "metadata": {},
   "source": [
    "**清除列名里的特殊字符，避免后续软件包里出错**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e1411a-2ee4-4cc2-9651-eb52eef76a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_char = [\"'\", '/', ' ']\n",
    "def normalize_feature_name(name):\n",
    "    for c in special_char:\n",
    "        name = name.replace(c, '_')\n",
    "    name = name.replace('(', '').replace(')', '')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5555df45-9f90-4205-bbfa-206fe3173a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_all_feats = [normalize_feature_name(col) for col in ori_all_feats]\n",
    "ori_cat_feats = [normalize_feature_name(col) for col in ori_cat_feats]\n",
    "ori_num_feats = [normalize_feature_name(col) for col in ori_num_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaefc58e-e33a-4f83-99cd-18305e0f58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df = pd.concat([train_df, test_df], axis=0, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc977d1-b8f5-495f-b000-326bcdfc9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df.columns = com_df.columns.map(normalize_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b7c4c5-d5ad-4fa9-a564-bea3bb5a0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ori_cat_feats:\n",
    "    com_df[col] = LabelEncoder().fit_transform(com_df[col])\n",
    "\n",
    "label2code = {\n",
    "    'Graduate': 0,\n",
    "    'Enrolled': 1,\n",
    "    'Dropout': 2,\n",
    "}\n",
    "\n",
    "code2label = {v: v for k, v in label2code.items()}\n",
    "com_df['Target'] = com_df['Target'].map(label2code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf71af8-8abc-4ff1-8834-d73a9094b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = com_df.loc[train_df.index].copy()\n",
    "train_df['Target'] = train_df['Target'].astype(int)\n",
    "test_df = com_df.loc[test_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a7dcd6-eba8-4fa7-84ba-dcbf0b7d9467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76518, 51012)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape[0], test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3c98994-11cb-4799-ac55-91c25d9b8922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del com_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99191ae6-afe7-47da-bd37-5323df94dca2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CV(交叉验证)\n",
    "\n",
    "CV是一切试验的基础，有了CV才知道哪些优化是正向的，哪些是没用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aae7a4cf-65ad-4f38-b5d5-9431d3bd0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_cv(params, train_df, test_df, feat_cols, cat_feat_cols, target_col, stratified=False, nfold=5, num_boost_round=10000):\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        folds = KFold(n_splits=nfold, shuffle=True, random_state=42)\n",
    "        \n",
    "    target = train_df[target_col]\n",
    "\n",
    "    oof = np.zeros((train_df.shape[0], 3), dtype=np.float64)\n",
    "    pred = np.zeros((test_df.shape[0], 3), dtype=np.float64)\n",
    "\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds.split(train_df.index, train_df[target_col].astype(int))):\n",
    "        print(f'fold={i}', '- ' * 20)\n",
    "        trn_data = lgb.Dataset(train_df.loc[trn_idx, feat_cols], label=target.loc[trn_idx], categorical_feature=cat_feat_cols)\n",
    "        val_data = lgb.Dataset(train_df.loc[val_idx, feat_cols], label=target.loc[val_idx], categorical_feature=cat_feat_cols)\n",
    "    \n",
    "        model = lgb.train(params, trn_data, num_boost_round, valid_sets=val_data, callbacks=[lgb.log_evaluation(200)])\n",
    "    \n",
    "        oof[val_idx] = model.predict(train_df.loc[val_idx, feat_cols], num_iteration=model.best_iteration)\n",
    "        pred += model.predict(test_df[feat_cols], num_iteration=model.best_iteration) / nfold\n",
    "\n",
    "    cv = accuracy_score(target, oof.argmax(axis=-1))\n",
    "    return cv, oof, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2496b2f-fd73-48bf-b708-1eb28148a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_threads': 16,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    'metric': 'multi_error',\n",
    "    'early_stopping_rounds': 400,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f38e769d-b1fc-4bc6-99fa-6bf412ccd558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=0 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1372\n",
      "[LightGBM] [Info] Number of data points in the train set: 61214, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.746287\n",
      "[LightGBM] [Info] Start training from score -1.635907\n",
      "[LightGBM] [Info] Start training from score -1.105333\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.172569\n",
      "[400]\tvalid_0's multi_error: 0.167734\n",
      "[600]\tvalid_0's multi_error: 0.166231\n",
      "[800]\tvalid_0's multi_error: 0.166035\n",
      "[1000]\tvalid_0's multi_error: 0.16499\n",
      "[1200]\tvalid_0's multi_error: 0.164663\n",
      "[1400]\tvalid_0's multi_error: 0.164597\n",
      "Early stopping, best iteration is:\n",
      "[1110]\tvalid_0's multi_error: 0.164532\n",
      "fold=1 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1374\n",
      "[LightGBM] [Info] Number of data points in the train set: 61214, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.748012\n",
      "[LightGBM] [Info] Start training from score -1.626225\n",
      "[LightGBM] [Info] Start training from score -1.108594\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.169564\n",
      "[400]\tvalid_0's multi_error: 0.165708\n",
      "[600]\tvalid_0's multi_error: 0.163944\n",
      "[800]\tvalid_0's multi_error: 0.161918\n",
      "[1000]\tvalid_0's multi_error: 0.161788\n",
      "[1200]\tvalid_0's multi_error: 0.162311\n",
      "Early stopping, best iteration is:\n",
      "[943]\tvalid_0's multi_error: 0.161265\n",
      "fold=2 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1374\n",
      "[LightGBM] [Info] Number of data points in the train set: 61214, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.746873\n",
      "[LightGBM] [Info] Start training from score -1.634231\n",
      "[LightGBM] [Info] Start training from score -1.105481\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.176947\n",
      "[400]\tvalid_0's multi_error: 0.17466\n",
      "[600]\tvalid_0's multi_error: 0.172439\n",
      "[800]\tvalid_0's multi_error: 0.172112\n",
      "[1000]\tvalid_0's multi_error: 0.171524\n",
      "[1200]\tvalid_0's multi_error: 0.170544\n",
      "[1400]\tvalid_0's multi_error: 0.170478\n",
      "[1600]\tvalid_0's multi_error: 0.170609\n",
      "[1800]\tvalid_0's multi_error: 0.16989\n",
      "[2000]\tvalid_0's multi_error: 0.170609\n",
      "Early stopping, best iteration is:\n",
      "[1797]\tvalid_0's multi_error: 0.16989\n",
      "fold=3 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1372\n",
      "[LightGBM] [Info] Number of data points in the train set: 61215, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.744273\n",
      "[LightGBM] [Info] Start training from score -1.631738\n",
      "[LightGBM] [Info] Start training from score -1.110692\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.177285\n",
      "[400]\tvalid_0's multi_error: 0.173887\n",
      "[600]\tvalid_0's multi_error: 0.172319\n",
      "[800]\tvalid_0's multi_error: 0.172188\n",
      "[1000]\tvalid_0's multi_error: 0.171339\n",
      "[1200]\tvalid_0's multi_error: 0.171143\n",
      "[1400]\tvalid_0's multi_error: 0.170751\n",
      "[1600]\tvalid_0's multi_error: 0.170816\n",
      "[1800]\tvalid_0's multi_error: 0.170163\n",
      "[2000]\tvalid_0's multi_error: 0.170163\n",
      "Early stopping, best iteration is:\n",
      "[1730]\tvalid_0's multi_error: 0.169901\n",
      "fold=4 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1370\n",
      "[LightGBM] [Info] Number of data points in the train set: 61215, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.745580\n",
      "[LightGBM] [Info] Start training from score -1.639368\n",
      "[LightGBM] [Info] Start training from score -1.104313\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.174149\n",
      "[400]\tvalid_0's multi_error: 0.170032\n",
      "[600]\tvalid_0's multi_error: 0.168072\n",
      "[800]\tvalid_0's multi_error: 0.166961\n",
      "[1000]\tvalid_0's multi_error: 0.16683\n",
      "[1200]\tvalid_0's multi_error: 0.166765\n",
      "[1400]\tvalid_0's multi_error: 0.165784\n",
      "[1600]\tvalid_0's multi_error: 0.165196\n",
      "[1800]\tvalid_0's multi_error: 0.164739\n",
      "[2000]\tvalid_0's multi_error: 0.164543\n",
      "[2200]\tvalid_0's multi_error: 0.165131\n",
      "Early stopping, best iteration is:\n",
      "[1957]\tvalid_0's multi_error: 0.164282\n"
     ]
    }
   ],
   "source": [
    "cv, oof, pred = lgb_cv(params, train_df, test_df, ori_all_feats, ori_cat_feats, 'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e0aa5ac-8c20-43f6-a486-9d1cf15f5674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8340259808149716\n"
     ]
    }
   ],
   "source": [
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c7f87b51-a6c3-486d-a9da-3ff4269e92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_threads': 16,\n",
    "    'learning_rate': 0.05,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_error',\n",
    "    'early_stopping_rounds': 400,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fa4c67c-ebad-414b-a927-e58ba43c7f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=0 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1372\n",
      "[LightGBM] [Info] Number of data points in the train set: 61214, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.746287\n",
      "[LightGBM] [Info] Start training from score -1.635907\n",
      "[LightGBM] [Info] Start training from score -1.105333\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.165708\n",
      "[400]\tvalid_0's multi_error: 0.164597\n",
      "[600]\tvalid_0's multi_error: 0.164401\n",
      "[800]\tvalid_0's multi_error: 0.16512\n",
      "Early stopping, best iteration is:\n",
      "[444]\tvalid_0's multi_error: 0.163879\n",
      "fold=1 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1374\n",
      "[LightGBM] [Info] Number of data points in the train set: 61214, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.748012\n",
      "[LightGBM] [Info] Start training from score -1.626225\n",
      "[LightGBM] [Info] Start training from score -1.108594\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.163879\n",
      "[400]\tvalid_0's multi_error: 0.163487\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's multi_error: 0.162637\n",
      "fold=2 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1374\n",
      "[LightGBM] [Info] Number of data points in the train set: 61214, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.746873\n",
      "[LightGBM] [Info] Start training from score -1.634231\n",
      "[LightGBM] [Info] Start training from score -1.105481\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.172308\n",
      "[400]\tvalid_0's multi_error: 0.171328\n",
      "[600]\tvalid_0's multi_error: 0.172308\n",
      "[800]\tvalid_0's multi_error: 0.172569\n",
      "Early stopping, best iteration is:\n",
      "[410]\tvalid_0's multi_error: 0.170674\n",
      "fold=3 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1372\n",
      "[LightGBM] [Info] Number of data points in the train set: 61215, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.744273\n",
      "[LightGBM] [Info] Start training from score -1.631738\n",
      "[LightGBM] [Info] Start training from score -1.110692\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.172188\n",
      "[400]\tvalid_0's multi_error: 0.17147\n",
      "[600]\tvalid_0's multi_error: 0.171992\n",
      "[800]\tvalid_0's multi_error: 0.171927\n",
      "Early stopping, best iteration is:\n",
      "[521]\tvalid_0's multi_error: 0.170816\n",
      "fold=4 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1370\n",
      "[LightGBM] [Info] Number of data points in the train set: 61215, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.745580\n",
      "[LightGBM] [Info] Start training from score -1.639368\n",
      "[LightGBM] [Info] Start training from score -1.104313\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.167876\n",
      "[400]\tvalid_0's multi_error: 0.166699\n",
      "[600]\tvalid_0's multi_error: 0.166765\n",
      "[800]\tvalid_0's multi_error: 0.166634\n",
      "Early stopping, best iteration is:\n",
      "[451]\tvalid_0's multi_error: 0.165784\n"
     ]
    }
   ],
   "source": [
    "cv, oof, pred = lgb_cv(params, train_df, test_df, ori_all_feats, ori_cat_feats, 'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1c7ec18c-7a2f-48d0-b7ae-13a6ec39ba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8332418515904755\n"
     ]
    }
   ],
   "source": [
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0d89a-f32a-4c24-b90f-ae46d61f66fd",
   "metadata": {},
   "source": [
    "## AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "839b261b-3615-497a-8559-2262fe36e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.core.metrics import make_scorer\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ba684-88f5-44f5-98cc-06731dd8801b",
   "metadata": {},
   "source": [
    "### hack\n",
    "\n",
    "AutoGluon在计算 LightGBM 模型 metrics 时有bug，需要手动修复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cd2504-8b1c-4023-964a-abeeaf2351b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jiazhuang/opt/miniconda3/envs/py3.10/lib/python3.10/site-packages/autogluon/tabular/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import autogluon.tabular\n",
    "print(autogluon.tabular.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5add45c-3300-49d2-9658-effc580800ff",
   "metadata": {},
   "source": [
    "将文件`/<miniconda按照目录>/envs/py38/lib/python3.8/site-packages/autogluon/tabular/models/lgb/lgb_utils.py`的41行替换为：\n",
    "```python\n",
    "    if problem_type in [REGRESSION, QUANTILE]:\n",
    "        # TODO: Might not work for custom quantile metrics\n",
    "        def function_template(y_hat, data):\n",
    "            y_true = data.get_label()\n",
    "            return metric.name, metric(y_true, y_hat), is_higher_better  \n",
    "\n",
    "    elif needs_pred_proba:\n",
    "```\n",
    "重启 notebook kernel 生效！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8355c271-359a-41bb-9726-87dae787dc09",
   "metadata": {},
   "source": [
    "### 单模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a27492b-5008-48ca-884e-75bf80e6738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autogluon根据数据集的dtype，自动处理 数值/类别 特征\n",
    "for col in ori_cat_feats:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5d69b-a7b3-4a1a-af8a-7c1da6bf6bec",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00afd26c-6839-4c7b-b0a8-1b72be6c2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'GBM':{}  # 只使用 LightGBM 模型，默参\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8ad666e-8ed2-410e-a757-6342109db454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240626_153736\"\n",
      "Presets specified: ['medium_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'auto_stack': False,\n",
      " 'num_bag_folds': 5,\n",
      " 'num_bag_sets': 1,\n",
      " 'num_stack_levels': 0}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': False,\n",
      " 'calibrate': 'auto',\n",
      " 'ds_args': {'clean_up_fits': True,\n",
      "             'detection_time_frac': 0.25,\n",
      "             'holdout_data': None,\n",
      "             'holdout_frac': 0.1111111111111111,\n",
      "             'memory_safe_fits': True,\n",
      "             'n_folds': 2,\n",
      "             'n_repeats': 1,\n",
      "             'validation_procedure': 'holdout'},\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'included_model_types': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': 5,\n",
      " 'num_bag_sets': 1,\n",
      " 'num_stack_levels': 0,\n",
      " 'pseudo_data': None,\n",
      " 'refit_full': False,\n",
      " 'save_bag_folds': None,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Saving AutogluonModels/ag-20240626_153736/learner.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/predictor.pkl\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240626_153736\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Sun Jan 19 18:21:42 CST 2020\n",
      "CPU Count:          64\n",
      "GPU Count:          0\n",
      "Memory Avail:       94.54 GB / 251.62 GB (37.6%)\n",
      "Disk Space Avail:   121.86 GB / 879.22 GB (13.9%)\n",
      "===================================================\n",
      "Train Data Rows:    76518\n",
      "Train Data Columns: 36\n",
      "Label Column:       Target\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    96772.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11.82 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('category', 'category') : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t\t\t('float64', 'float')     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int64', 'int')         : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t\t\t('float', [])    :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])      : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t26 features in original data used to generate 26 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t0.0s = Fit runtime\n",
      "\t\t\t\t10 features in original data used to generate 10 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t10 features in original data used to generate 10 features in processed data.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t('float64', 'float')     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int64', 'int')         : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t('float', [])    :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])      : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t('float64', 'float')     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int64', 'int')         : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t('int8', 'int')          :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t36 features in original data used to generate 36 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 11.83 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.85s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutogluonModels/ag-20240626_153736/learner.pkl\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': {},\n",
      "}\n",
      "Saving AutogluonModels/ag-20240626_153736/utils/data/X.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/utils/data/y.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting LightGBM_BAG_L1 with 'num_gpus': 0, 'num_cpus': 64\n",
      "Saving AutogluonModels/ag-20240626_153736/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240626_153736/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=12, gpus=0, memory=0.10%)\n",
      "Saving AutogluonModels/ag-20240626_153736/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/models/LightGBM_BAG_L1/model.pkl\n",
      "\t0.8327\t = Validation score   (accuracy)\n",
      "\t244.74s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Saving AutogluonModels/ag-20240626_153736/models/trainer.pkl\n",
      "Loading: AutogluonModels/ag-20240626_153736/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 64\n",
      "Saving AutogluonModels/ag-20240626_153736/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240626_153736/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Ensemble size: 1\n",
      "Ensemble weights: \n",
      "[1.]\n",
      "Saving AutogluonModels/ag-20240626_153736/models/WeightedEnsemble_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/models/WeightedEnsemble_L2/model.pkl\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.8327\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Saving AutogluonModels/ag-20240626_153736/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/models/trainer.pkl\n",
      "AutoGluon training complete, total runtime = 274.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: AutogluonModels/ag-20240626_153736/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/learner.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/predictor.pkl\n",
      "Saving AutogluonModels/ag-20240626_153736/__version__ with contents \"1.1.0\"\n",
      "Saving AutogluonModels/ag-20240626_153736/metadata.json\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240626_153736\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label='Target',\n",
    "    problem_type='multiclass',\n",
    "    eval_metric='accuracy',\n",
    "    verbosity=3,\n",
    ").fit(\n",
    "    train_df, \n",
    "    hyperparameters=hyperparameters,\n",
    "    presets='medium_quality',\n",
    "    num_bag_sets=1,\n",
    "    num_bag_folds=5,\n",
    "    num_stack_levels=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "935743c1-a382-40c3-91b6-eb413ba7f7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.832719</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.777771</td>\n",
       "      <td>244.744118</td>\n",
       "      <td>0.777771</td>\n",
       "      <td>244.744118</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.832719</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.786112</td>\n",
       "      <td>244.761742</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>0.017624</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_val eval_metric  pred_time_val    fit_time  \\\n",
       "0      LightGBM_BAG_L1   0.832719    accuracy       0.777771  244.744118   \n",
       "1  WeightedEnsemble_L2   0.832719    accuracy       0.786112  244.761742   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.777771         244.744118            1       True   \n",
       "1                0.008341           0.017624            2       True   \n",
       "\n",
       "   fit_order  \n",
       "0          1  \n",
       "1          2  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c57ff-47ff-43c8-b918-0664854db2b4",
   "metadata": {},
   "source": [
    "#### LightGBM 手动设置超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd37edc8-54b7-4f32-bcc5-cd721a7691fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_threads': 12,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    'metric': 'multi_error',\n",
    "    'early_stopping_rounds': 400,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82441b62-29e0-4c1b-bb8e-56b85eaf915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'GBM': params  # 只使用 LightGBM 模型，默参\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd20e699-a75f-4905-bb22-439d5a7aa72d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240627_134523\"\n",
      "Presets specified: ['medium_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'auto_stack': False,\n",
      " 'num_bag_folds': 5,\n",
      " 'num_bag_sets': 1,\n",
      " 'num_stack_levels': 0}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': False,\n",
      " 'calibrate': 'auto',\n",
      " 'ds_args': {'clean_up_fits': True,\n",
      "             'detection_time_frac': 0.25,\n",
      "             'holdout_data': None,\n",
      "             'holdout_frac': 0.1111111111111111,\n",
      "             'memory_safe_fits': True,\n",
      "             'n_folds': 2,\n",
      "             'n_repeats': 1,\n",
      "             'validation_procedure': 'holdout'},\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'included_model_types': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': 5,\n",
      " 'num_bag_sets': 1,\n",
      " 'num_stack_levels': 0,\n",
      " 'pseudo_data': None,\n",
      " 'refit_full': False,\n",
      " 'save_bag_folds': None,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Saving AutogluonModels/ag-20240627_134523/learner.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/predictor.pkl\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240627_134523\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Sun Jan 19 18:21:42 CST 2020\n",
      "CPU Count:          64\n",
      "GPU Count:          0\n",
      "Memory Avail:       93.15 GB / 251.62 GB (37.0%)\n",
      "Disk Space Avail:   107.31 GB / 879.22 GB (12.2%)\n",
      "===================================================\n",
      "Train Data Rows:    76518\n",
      "Train Data Columns: 36\n",
      "Label Column:       Target\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    95368.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11.82 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('category', 'category') : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t\t\t('float64', 'float')     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int64', 'int')         : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t\t\t('float', [])    :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])      : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t26 features in original data used to generate 26 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t0.0s = Fit runtime\n",
      "\t\t\t\t10 features in original data used to generate 10 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t10 features in original data used to generate 10 features in processed data.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t('float64', 'float')     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int64', 'int')         : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t('float', [])    :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])      : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t('float64', 'float')     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int64', 'int')         : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t('int8', 'int')          :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t36 features in original data used to generate 36 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 11.83 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutogluonModels/ag-20240627_134523/learner.pkl\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': {'num_threads': 12, 'learning_rate': 0.01, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 31, 'min_data_in_leaf': 20, 'bagging_freq': 1, 'bagging_fraction': 0.8, 'feature_fraction': 0.8, 'metric': 'multi_error', 'early_stopping_rounds': 400},\n",
      "}\n",
      "Saving AutogluonModels/ag-20240627_134523/utils/data/X.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/utils/data/y.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBM_BAG_L1: \t{'num_threads': 12, 'learning_rate': 0.01, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 31, 'min_data_in_leaf': 20, 'bagging_freq': 1, 'bagging_fraction': 0.8, 'feature_fraction': 0.8, 'metric': 'multi_error', 'early_stopping_rounds': 400, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting LightGBM_BAG_L1 with 'num_gpus': 0, 'num_cpus': 64\n",
      "Saving AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=12, gpus=0, memory=0.10%)\n",
      "Saving AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/model.pkl\n",
      "\t0.833\t = Validation score   (accuracy)\n",
      "\t30.89s\t = Training   runtime\n",
      "\t3.09s\t = Validation runtime\n",
      "Saving AutogluonModels/ag-20240627_134523/models/trainer.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 64\n",
      "Saving AutogluonModels/ag-20240627_134523/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Ensemble size: 1\n",
      "Ensemble weights: \n",
      "[1.]\n",
      "Saving AutogluonModels/ag-20240627_134523/models/WeightedEnsemble_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/models/WeightedEnsemble_L2/model.pkl\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.833\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Saving AutogluonModels/ag-20240627_134523/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/models/trainer.pkl\n",
      "AutoGluon training complete, total runtime = 33.88s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/learner.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/predictor.pkl\n",
      "Saving AutogluonModels/ag-20240627_134523/__version__ with contents \"1.1.0\"\n",
      "Saving AutogluonModels/ag-20240627_134523/metadata.json\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240627_134523\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label='Target',\n",
    "    problem_type='multiclass',\n",
    "    eval_metric='accuracy',\n",
    "    verbosity=3,\n",
    ").fit(\n",
    "    train_df, \n",
    "    hyperparameters=hyperparameters,\n",
    "    presets='medium_quality',\n",
    "    num_bag_sets=1,\n",
    "    num_bag_folds=5,\n",
    "    num_stack_levels=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52b78cf3-4f6e-4142-827b-20b4d534b4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.833033</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>3.086690</td>\n",
       "      <td>30.892312</td>\n",
       "      <td>3.086690</td>\n",
       "      <td>30.892312</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.833033</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>3.094825</td>\n",
       "      <td>30.906410</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.014097</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_val eval_metric  pred_time_val   fit_time  \\\n",
       "0      LightGBM_BAG_L1   0.833033    accuracy       3.086690  30.892312   \n",
       "1  WeightedEnsemble_L2   0.833033    accuracy       3.094825  30.906410   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                3.086690          30.892312            1       True   \n",
       "1                0.008135           0.014097            2       True   \n",
       "\n",
       "   fit_order  \n",
       "0          1  \n",
       "1          2  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(extra_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "afb66e52-bc09-47bf-9f96-38fa10884888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/info.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F5/info.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F5/model.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F1/info.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F4/info.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F4/model.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F2/info.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F3/info.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/LightGBM_BAG_L1/S1F3/model.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/WeightedEnsemble_L2/info.pkl\n",
      "Loading: AutogluonModels/ag-20240627_134523/models/WeightedEnsemble_L2/model.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "      <th>...</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>hyperparameters_fit</th>\n",
       "      <th>ag_args_fit</th>\n",
       "      <th>features</th>\n",
       "      <th>compile_time</th>\n",
       "      <th>child_hyperparameters</th>\n",
       "      <th>child_hyperparameters_fit</th>\n",
       "      <th>child_ag_args_fit</th>\n",
       "      <th>ancestors</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.833033</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>3.086690</td>\n",
       "      <td>30.892312</td>\n",
       "      <td>3.086690</td>\n",
       "      <td>30.892312</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>{'use_orig_features': True, 'max_base_models':...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Curricular_units_1st_sem_evaluations, Scholar...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'learning_rate': 0.01, 'num_threads': 12, 'ob...</td>\n",
       "      <td>{'num_boost_round': 1044}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.833033</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>3.094825</td>\n",
       "      <td>30.906410</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.014097</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>{'use_orig_features': False, 'max_base_models'...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBM_BAG_L1_1, LightGBM_BAG_L1_2, LightGB...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ensemble_size': 25, 'subsample_size': 1000000}</td>\n",
       "      <td>{'ensemble_size': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBM_BAG_L1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_val eval_metric  pred_time_val   fit_time  \\\n",
       "0      LightGBM_BAG_L1   0.833033    accuracy       3.086690  30.892312   \n",
       "1  WeightedEnsemble_L2   0.833033    accuracy       3.094825  30.906410   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                3.086690          30.892312            1       True   \n",
       "1                0.008135           0.014097            2       True   \n",
       "\n",
       "   fit_order  ...                                    hyperparameters  \\\n",
       "0          1  ...  {'use_orig_features': True, 'max_base_models':...   \n",
       "1          2  ...  {'use_orig_features': False, 'max_base_models'...   \n",
       "\n",
       "   hyperparameters_fit                                        ag_args_fit  \\\n",
       "0                   {}  {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "1                   {}  {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "\n",
       "                                            features  compile_time  \\\n",
       "0  [Curricular_units_1st_sem_evaluations, Scholar...          None   \n",
       "1  [LightGBM_BAG_L1_1, LightGBM_BAG_L1_2, LightGB...          None   \n",
       "\n",
       "                               child_hyperparameters  \\\n",
       "0  {'learning_rate': 0.01, 'num_threads': 12, 'ob...   \n",
       "1   {'ensemble_size': 25, 'subsample_size': 1000000}   \n",
       "\n",
       "   child_hyperparameters_fit  \\\n",
       "0  {'num_boost_round': 1044}   \n",
       "1       {'ensemble_size': 1}   \n",
       "\n",
       "                                   child_ag_args_fit          ancestors  \\\n",
       "0  {'max_memory_usage_ratio': 1.0, 'max_time_limi...                 []   \n",
       "1  {'max_memory_usage_ratio': 1.0, 'max_time_limi...  [LightGBM_BAG_L1]   \n",
       "\n",
       "             descendants  \n",
       "0  [WeightedEnsemble_L2]  \n",
       "1                     []  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_df = predictor.leaderboard(extra_info=True)\n",
    "lb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "90524e6c-8592-4038-9ee5-b90aa9be7e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'num_threads': 12,\n",
       " 'objective': 'multiclass',\n",
       " 'num_class': 3,\n",
       " 'num_leaves': 31,\n",
       " 'min_data_in_leaf': 20,\n",
       " 'bagging_freq': 1,\n",
       " 'bagging_fraction': 0.8,\n",
       " 'feature_fraction': 0.8,\n",
       " 'metric': 'multi_error',\n",
       " 'early_stopping_rounds': 400}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_df.iloc[0].child_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9fb82dbe-fa96-490f-a0dc-f97aebdde563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_boost_round': 1044}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_df.iloc[0].child_hyperparameters_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b1075-42d6-4a12-a759-bf00e78f7ebb",
   "metadata": {},
   "source": [
    "#### LightGBM 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8baba92d-db27-4f51-9181-3a2c0b19c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.common import space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cf737ab8-658b-48df-b871-17b93ffa60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # 'num_threads': 12,\n",
    "    'learning_rate': space.Real(1e-3, 5e-1, default=5e-2, log=True),\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'num_leaves': space.Int(lower=2, upper=100, default=31),\n",
    "    'min_data_in_leaf': space.Int(lower=10, upper=100, default=20),\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_fraction': space.Real(0.0, 1.0, default=0.8),\n",
    "    'feature_fraction': space.Real(0.0, 1.0, default=0.8),\n",
    "    'metric': 'multi_error',\n",
    "    'early_stopping_rounds': 400,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9e7a6861-5646-408b-8c5a-924c52bfdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'GBM': params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "60cba60a-fd2a-48d0-bcaf-e2fabce87945",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tune_kwargs = {\n",
    "    'num_trials': 100,\n",
    "    'searcher': 'auto',\n",
    "    'scheduler' : 'local',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bb15a55e-03c2-44af-ab2f-a8630c33e5e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240628_021311\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'holdout_frac': 0.3,\n",
      " 'hyperparameter_tune_kwargs': {'num_trials': 100,\n",
      "                                'scheduler': 'local',\n",
      "                                'searcher': 'auto'},\n",
      " 'num_stack_levels': 0}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': False,\n",
      " 'calibrate': 'auto',\n",
      " 'ds_args': {'clean_up_fits': True,\n",
      "             'detection_time_frac': 0.25,\n",
      "             'holdout_data': None,\n",
      "             'holdout_frac': 0.1111111111111111,\n",
      "             'memory_safe_fits': True,\n",
      "             'n_folds': 2,\n",
      "             'n_repeats': 1,\n",
      "             'validation_procedure': 'holdout'},\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': 0.3,\n",
      " 'hyperparameter_tune_kwargs': {'num_trials': 100,\n",
      "                                'scheduler': 'local',\n",
      "                                'searcher': 'auto'},\n",
      " 'included_model_types': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': 0,\n",
      " 'pseudo_data': None,\n",
      " 'refit_full': False,\n",
      " 'save_bag_folds': None,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Saving AutogluonModels/ag-20240628_021311/learner.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/predictor.pkl\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240628_021311\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Sun Jan 19 18:21:42 CST 2020\n",
      "CPU Count:          64\n",
      "GPU Count:          0\n",
      "Memory Avail:       93.74 GB / 251.62 GB (37.3%)\n",
      "Disk Space Avail:   106.77 GB / 879.22 GB (12.1%)\n",
      "===================================================\n",
      "Train Data Rows:    76518\n",
      "Train Data Columns: 36\n",
      "Label Column:       Target\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    96000.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11.82 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('category', 'category') : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t\t\t('float64', 'float')     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int64', 'int')         : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t\t\t('float', [])    :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])      : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t26 features in original data used to generate 26 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t0.0s = Fit runtime\n",
      "\t\t\t\t10 features in original data used to generate 10 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t10 features in original data used to generate 10 features in processed data.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t('float64', 'float')     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int64', 'int')         : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t('float', [])    :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])      : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t('float64', 'float')     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int64', 'int')         : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t('int8', 'int')          :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t36 features in original data used to generate 36 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 11.83 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutogluonModels/ag-20240628_021311/learner.pkl\n",
      "Automatically generating train/validation split with holdout_frac=0.3, Train Rows: 53562, Val Rows: 22956\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': {'learning_rate': Real: lower=0.001, upper=0.5, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': Int: lower=2, upper=100, 'min_data_in_leaf': Int: lower=10, upper=100, 'bagging_freq': 1, 'bagging_fraction': Real: lower=0.0, upper=1.0, 'feature_fraction': Real: lower=0.0, upper=1.0, 'metric': 'multi_error', 'early_stopping_rounds': 400},\n",
      "}\n",
      "Saving AutogluonModels/ag-20240628_021311/utils/data/X.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/utils/data/y.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/utils/data/X_val.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/utils/data/y_val.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBM: \t{'learning_rate': Real: lower=0.001, upper=0.5, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': Int: lower=2, upper=100, 'min_data_in_leaf': Int: lower=10, upper=100, 'bagging_freq': 1, 'bagging_fraction': Real: lower=0.0, upper=1.0, 'feature_fraction': Real: lower=0.0, upper=1.0, 'metric': 'multi_error', 'early_stopping_rounds': 400, 'ag_args': {'hyperparameter_tune_kwargs': {'num_trials': 100, 'searcher': 'auto', 'scheduler': 'local'}, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "Fitting 1 L1 models ...\n",
      "Hyperparameter tuning model: LightGBM ...\n",
      "\tFitting LightGBM with 'num_gpus': 0, 'num_cpus': 32\n",
      "Starting generic AbstractModel hyperparameter tuning for LightGBM model...\n",
      "\tHyperparameter search space for LightGBM: \n",
      "learning_rate:   Real: lower=0.001, upper=0.5\n",
      "num_leaves:   Int: lower=2, upper=100\n",
      "min_data_in_leaf:   Int: lower=10, upper=100\n",
      "bagging_fraction:   Real: lower=0.0, upper=1.0\n",
      "feature_fraction:   Real: lower=0.0, upper=1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a558c18c6e147bf8c2c9dc433a50c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 31, 'min_data_in_leaf': 20, 'bagging_freq': 1, 'bagging_fraction': 0.8, 'feature_fraction': 0.8, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.042348718392737926, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 11, 'min_data_in_leaf': 77, 'bagging_freq': 1, 'bagging_fraction': 0.5488135039273248, 'feature_fraction': 0.7151893663724195, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.006353850298631268, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 90, 'min_data_in_leaf': 98, 'bagging_freq': 1, 'bagging_fraction': 0.6235636967859723, 'feature_fraction': 0.3843817072926998, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.166972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T3/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.1556033839305467, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 90, 'min_data_in_leaf': 56, 'bagging_freq': 1, 'bagging_fraction': 0.2726562945801132, 'feature_fraction': 0.47766511732134986, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T4/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.008139957109567855, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 82, 'min_data_in_leaf': 30, 'bagging_freq': 1, 'bagging_fraction': 0.3927847961008297, 'feature_fraction': 0.8360787635373775, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.167233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T5/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.22291366459641138, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 90, 'min_data_in_leaf': 92, 'bagging_freq': 1, 'bagging_fraction': 0.832619845547938, 'feature_fraction': 0.7781567509498505, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T6/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.06796346121798984, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 34, 'min_data_in_leaf': 49, 'bagging_freq': 1, 'bagging_fraction': 0.8009107519796442, 'feature_fraction': 0.5204774795512048, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T7/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.11155288713260661, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 25, 'min_data_in_leaf': 84, 'bagging_freq': 1, 'bagging_fraction': 0.5820197920751071, 'feature_fraction': 0.5373732294490107, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T8/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.017026918768674876, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 2, 'min_data_in_leaf': 44, 'bagging_freq': 1, 'bagging_fraction': 0.26455561210462697, 'feature_fraction': 0.7742336894342167, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.177862\n",
      "[2000]\tvalid_set's multi_error: 0.174116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T9/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.044877431780743424, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 81, 'min_data_in_leaf': 27, 'bagging_freq': 1, 'bagging_fraction': 0.018789800436355142, 'feature_fraction': 0.6176354970758771, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T10/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.04514814213154827, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 67, 'min_data_in_leaf': 11, 'bagging_freq': 1, 'bagging_fraction': 0.9025984755294046, 'feature_fraction': 0.4499499899112276, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T11/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05791690760610934, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 48, 'min_data_in_leaf': 21, 'bagging_freq': 1, 'bagging_fraction': 0.09928035035897387, 'feature_fraction': 0.9698090677467488, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T12/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.04370355269950185, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 44, 'min_data_in_leaf': 22, 'bagging_freq': 1, 'bagging_fraction': 0.358152166969525, 'feature_fraction': 0.7506861412184562, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T13/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0018854537484233854, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 78, 'min_data_in_leaf': 13, 'bagging_freq': 1, 'bagging_fraction': 0.43860151346232035, 'feature_fraction': 0.9883738380592262, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T14/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.018136721685137126, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 25, 'min_data_in_leaf': 68, 'bagging_freq': 1, 'bagging_fraction': 0.6531083254653984, 'feature_fraction': 0.2532916025397821, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T15/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0071798932488217025, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 43, 'min_data_in_leaf': 79, 'bagging_freq': 1, 'bagging_fraction': 0.33800761483889175, 'feature_fraction': 0.6747523222590207, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.167538\n",
      "[2000]\tvalid_set's multi_error: 0.165578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T16/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0010880013579881585, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 38, 'min_data_in_leaf': 60, 'bagging_freq': 1, 'bagging_fraction': 0.9495710534507421, 'feature_fraction': 0.6625268669500443, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T17/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.018402445310508105, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 23, 'min_data_in_leaf': 87, 'bagging_freq': 1, 'bagging_fraction': 0.09609840789396307, 'feature_fraction': 0.9764594650133958, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T18/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.005798124133134792, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 25, 'min_data_in_leaf': 68, 'bagging_freq': 1, 'bagging_fraction': 0.7392635793983017, 'feature_fraction': 0.039187792254320675, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.214541\n",
      "[2000]\tvalid_set's multi_error: 0.193021\n",
      "[3000]\tvalid_set's multi_error: 0.185137\n",
      "[4000]\tvalid_set's multi_error: 0.181783\n",
      "[5000]\tvalid_set's multi_error: 0.1793\n",
      "[6000]\tvalid_set's multi_error: 0.177296\n",
      "[7000]\tvalid_set's multi_error: 0.176076\n",
      "[8000]\tvalid_set's multi_error: 0.174377\n",
      "[9000]\tvalid_set's multi_error: 0.17368\n",
      "[10000]\tvalid_set's multi_error: 0.172852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T19/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.23789100148510528, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 84, 'min_data_in_leaf': 77, 'bagging_freq': 1, 'bagging_fraction': 0.4808935308361628, 'feature_fraction': 0.6886611828057704, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T20/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.023642421622859816, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 43, 'min_data_in_leaf': 24, 'bagging_freq': 1, 'bagging_fraction': 0.5651888666048753, 'feature_fraction': 0.865102561305385, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T21/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.005617642096133367, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 4, 'min_data_in_leaf': 21, 'bagging_freq': 1, 'bagging_fraction': 0.9211576102371998, 'feature_fraction': 0.08311249263060239, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.186008\n",
      "[2000]\tvalid_set's multi_error: 0.180563\n",
      "[3000]\tvalid_set's multi_error: 0.175945\n",
      "[4000]\tvalid_set's multi_error: 0.173506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T22/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.006040854339990919, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 48, 'min_data_in_leaf': 29, 'bagging_freq': 1, 'bagging_fraction': 0.13179786240439217, 'feature_fraction': 0.7163272041185655, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.169629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T23/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.17269679360003765, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 26, 'min_data_in_leaf': 40, 'bagging_freq': 1, 'bagging_fraction': 0.5865129348100832, 'feature_fraction': 0.020107546187493552, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.171023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T24/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0964420899238991, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 42, 'min_data_in_leaf': 23, 'bagging_freq': 1, 'bagging_fraction': 0.6778165367962301, 'feature_fraction': 0.27000797319216485, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T25/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.2620479283097813, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 69, 'min_data_in_leaf': 62, 'bagging_freq': 1, 'bagging_fraction': 0.7561066938650409, 'feature_fraction': 0.3960982754233623, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T26/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.19250039208170594, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 88, 'min_data_in_leaf': 21, 'bagging_freq': 1, 'bagging_fraction': 0.952749011516985, 'feature_fraction': 0.44712537861762736, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T27/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.23882022310753243, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 31, 'min_data_in_leaf': 34, 'bagging_freq': 1, 'bagging_fraction': 0.8137978197024772, 'feature_fraction': 0.39650574084698464, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T28/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0906649751284636, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 85, 'min_data_in_leaf': 71, 'bagging_freq': 1, 'bagging_fraction': 0.8817353618548528, 'feature_fraction': 0.6925315900777659, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T29/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.043314878025372455, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 87, 'min_data_in_leaf': 80, 'bagging_freq': 1, 'bagging_fraction': 0.6439901992296374, 'feature_fraction': 0.4238550485581797, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T30/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.01083809723894249, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 20, 'min_data_in_leaf': 51, 'bagging_freq': 1, 'bagging_fraction': 0.7160745312286432, 'feature_fraction': 0.2879910043632805, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.170239\n",
      "[2000]\tvalid_set's multi_error: 0.167407\n",
      "[3000]\tvalid_set's multi_error: 0.167407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T31/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0017754741633347207, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 97, 'min_data_in_leaf': 11, 'bagging_freq': 1, 'bagging_fraction': 0.878452190276042, 'feature_fraction': 0.10286335869343821, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T32/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.007351262416410615, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 5, 'min_data_in_leaf': 92, 'bagging_freq': 1, 'bagging_fraction': 0.033625093498832026, 'feature_fraction': 0.9689617652602737, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.176947\n",
      "[2000]\tvalid_set's multi_error: 0.174334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T33/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.2554314574235273, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 71, 'min_data_in_leaf': 71, 'bagging_freq': 1, 'bagging_fraction': 0.36756187004789653, 'feature_fraction': 0.4358649252656268, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T34/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.30314889811793405, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 56, 'min_data_in_leaf': 20, 'bagging_freq': 1, 'bagging_fraction': 0.7038885835403663, 'feature_fraction': 0.10022688731230112, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T35/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.22031607648637247, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 29, 'min_data_in_leaf': 12, 'bagging_freq': 1, 'bagging_fraction': 0.9988470065678665, 'feature_fraction': 0.14944830465799375, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T36/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.19442350777316741, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 55, 'min_data_in_leaf': 30, 'bagging_freq': 1, 'bagging_fraction': 0.6155595642838442, 'feature_fraction': 0.12381998284944151, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T37/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0015370204507549887, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 25, 'min_data_in_leaf': 51, 'bagging_freq': 1, 'bagging_fraction': 0.5691007386145933, 'feature_fraction': 0.40718329722599966, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T38/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.2179414871180477, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 11, 'min_data_in_leaf': 41, 'bagging_freq': 1, 'bagging_fraction': 0.45354268267806885, 'feature_fraction': 0.7220555994703479, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T39/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.009366341715379745, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 99, 'min_data_in_leaf': 95, 'bagging_freq': 1, 'bagging_fraction': 0.855803342392611, 'feature_fraction': 0.011714084185001972, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.197508\n",
      "[2000]\tvalid_set's multi_error: 0.184222\n",
      "[3000]\tvalid_set's multi_error: 0.179387\n",
      "[4000]\tvalid_set's multi_error: 0.176991\n",
      "[5000]\tvalid_set's multi_error: 0.175815\n",
      "[6000]\tvalid_set's multi_error: 0.173637\n",
      "[7000]\tvalid_set's multi_error: 0.172547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T40/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0014017049293645642, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 2, 'min_data_in_leaf': 46, 'bagging_freq': 1, 'bagging_fraction': 0.17162967726144052, 'feature_fraction': 0.5210366062041293, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T41/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.008552514657227738, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 64, 'min_data_in_leaf': 90, 'bagging_freq': 1, 'bagging_fraction': 0.7936977033574206, 'feature_fraction': 0.22392468806038013, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.171589\n",
      "[2000]\tvalid_set's multi_error: 0.167538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T42/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0027829447752274524, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 38, 'min_data_in_leaf': 50, 'bagging_freq': 1, 'bagging_fraction': 0.7044144019235328, 'feature_fraction': 0.03183892953130785, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T43/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.06805267512143187, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 31, 'min_data_in_leaf': 40, 'bagging_freq': 1, 'bagging_fraction': 0.9923963988886326, 'feature_fraction': 0.26726253788147825, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T44/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.011879186536249427, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 71, 'min_data_in_leaf': 12, 'bagging_freq': 1, 'bagging_fraction': 0.7301220295167696, 'feature_fraction': 0.31194499547960186, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.166667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T45/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.01660461170757197, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 41, 'min_data_in_leaf': 95, 'bagging_freq': 1, 'bagging_fraction': 0.21874937373677183, 'feature_fraction': 0.569573534574738, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T46/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.02073286553547695, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 20, 'min_data_in_leaf': 81, 'bagging_freq': 1, 'bagging_fraction': 0.08529556585870057, 'feature_fraction': 0.056418332713966235, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.180998\n",
      "[2000]\tvalid_set's multi_error: 0.173767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T47/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.029119768707609014, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 23, 'min_data_in_leaf': 56, 'bagging_freq': 1, 'bagging_fraction': 0.9764043865930425, 'feature_fraction': 0.6176579160958802, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T48/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.06720829203139939, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 83, 'min_data_in_leaf': 38, 'bagging_freq': 1, 'bagging_fraction': 0.7438345453097878, 'feature_fraction': 0.4785963256187501, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T49/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.06160673326852021, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 96, 'min_data_in_leaf': 46, 'bagging_freq': 1, 'bagging_fraction': 0.9903389473967044, 'feature_fraction': 0.21689698439847394, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T50/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.2760678802280982, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 93, 'min_data_in_leaf': 66, 'bagging_freq': 1, 'bagging_fraction': 0.22921932308657944, 'feature_fraction': 0.8815853992105808, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T51/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0010003440804036725, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 73, 'min_data_in_leaf': 82, 'bagging_freq': 1, 'bagging_fraction': 0.32468297206646535, 'feature_fraction': 0.5197111936582762, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T52/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.06838711131024695, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 66, 'min_data_in_leaf': 65, 'bagging_freq': 1, 'bagging_fraction': 0.42545153795217594, 'feature_fraction': 0.8853376596095855, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T53/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0041615644888373104, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 88, 'min_data_in_leaf': 24, 'bagging_freq': 1, 'bagging_fraction': 0.48340861659875267, 'feature_fraction': 0.7887394275320365, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.169063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T54/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.018760453797426507, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 9, 'min_data_in_leaf': 93, 'bagging_freq': 1, 'bagging_fraction': 0.3136923925080297, 'feature_fraction': 0.9574508556589537, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.168671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T55/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.017831019095854835, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 81, 'min_data_in_leaf': 89, 'bagging_freq': 1, 'bagging_fraction': 0.42468546875150626, 'feature_fraction': 0.37416998033422555, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T56/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.002075940748858884, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 76, 'min_data_in_leaf': 36, 'bagging_freq': 1, 'bagging_fraction': 0.5867843464581688, 'feature_fraction': 0.8638556059232314, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T57/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.01172069211428234, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 36, 'min_data_in_leaf': 28, 'bagging_freq': 1, 'bagging_fraction': 0.1320681063451533, 'feature_fraction': 0.7168596811925937, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T58/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.00911565988073664, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 47, 'min_data_in_leaf': 28, 'bagging_freq': 1, 'bagging_fraction': 0.14484775934337724, 'feature_fraction': 0.48805628064895457, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.167277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T59/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.001679401941590302, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 82, 'min_data_in_leaf': 69, 'bagging_freq': 1, 'bagging_fraction': 0.7486636198505473, 'feature_fraction': 0.9037197397459334, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T60/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.00594805664341383, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 46, 'min_data_in_leaf': 83, 'bagging_freq': 1, 'bagging_fraction': 0.04561463715088643, 'feature_fraction': 0.20915702948084114, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.181173\n",
      "[2000]\tvalid_set's multi_error: 0.175597\n",
      "[3000]\tvalid_set's multi_error: 0.171807\n",
      "[4000]\tvalid_set's multi_error: 0.170761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T61/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0010591041777891583, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 9, 'min_data_in_leaf': 23, 'bagging_freq': 1, 'bagging_fraction': 0.06303828966937675, 'feature_fraction': 0.5556492427701452, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T62/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0014880356802293828, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 8, 'min_data_in_leaf': 18, 'bagging_freq': 1, 'bagging_fraction': 0.2817301057539491, 'feature_fraction': 0.5864101661863267, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T63/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0038856417245090665, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 58, 'min_data_in_leaf': 26, 'bagging_freq': 1, 'bagging_fraction': 0.7875421559550259, 'feature_fraction': 0.6064754600685757, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.16915\n",
      "[2000]\tvalid_set's multi_error: 0.167277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T64/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0853646571183463, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 89, 'min_data_in_leaf': 59, 'bagging_freq': 1, 'bagging_fraction': 0.8788697417774708, 'feature_fraction': 0.4922682415051587, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T65/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.1955569211746371, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 25, 'min_data_in_leaf': 67, 'bagging_freq': 1, 'bagging_fraction': 0.874287966624947, 'feature_fraction': 0.2930202845077967, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T66/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0025108903068004652, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 88, 'min_data_in_leaf': 61, 'bagging_freq': 1, 'bagging_fraction': 0.01323685775889949, 'feature_fraction': 0.34723351793221957, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.189449\n",
      "[2000]\tvalid_set's multi_error: 0.180911\n",
      "[3000]\tvalid_set's multi_error: 0.17808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T67/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.053200674728298494, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 85, 'min_data_in_leaf': 10, 'bagging_freq': 1, 'bagging_fraction': 0.47837030703998806, 'feature_fraction': 0.4973913654986627, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T68/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0032538222127483194, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 44, 'min_data_in_leaf': 13, 'bagging_freq': 1, 'bagging_fraction': 0.13690027168559893, 'feature_fraction': 0.8221177331942455, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.171763\n",
      "[2000]\tvalid_set's multi_error: 0.168583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T69/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.034378171013130794, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 12, 'min_data_in_leaf': 57, 'bagging_freq': 1, 'bagging_fraction': 0.9806996740240193, 'feature_fraction': 0.40612049298131103, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T70/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.009325299267022786, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 22, 'min_data_in_leaf': 41, 'bagging_freq': 1, 'bagging_fraction': 0.7888727777194641, 'feature_fraction': 0.4113724140379379, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.168017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T71/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.316097394561388, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 78, 'min_data_in_leaf': 93, 'bagging_freq': 1, 'bagging_fraction': 0.30183087592751656, 'feature_fraction': 0.7752197774666111, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T72/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.02753010535434901, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 14, 'min_data_in_leaf': 13, 'bagging_freq': 1, 'bagging_fraction': 0.9528706723868426, 'feature_fraction': 0.013948395933415347, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.180737\n",
      "[2000]\tvalid_set's multi_error: 0.17368\n",
      "[3000]\tvalid_set's multi_error: 0.172112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T73/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.06734637252882937, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 60, 'min_data_in_leaf': 34, 'bagging_freq': 1, 'bagging_fraction': 0.882859998043791, 'feature_fraction': 0.25062272019388543, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T74/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.17340897959446988, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 62, 'min_data_in_leaf': 50, 'bagging_freq': 1, 'bagging_fraction': 0.4321480499248166, 'feature_fraction': 0.7521345209879338, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T75/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0014107615996109843, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 75, 'min_data_in_leaf': 61, 'bagging_freq': 1, 'bagging_fraction': 0.27032790523871464, 'feature_fraction': 0.1314827992911276, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T76/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.015595025319047195, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 86, 'min_data_in_leaf': 82, 'bagging_freq': 1, 'bagging_fraction': 0.4585145522366506, 'feature_fraction': 0.5245926207548479, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T77/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.13435711962260233, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 73, 'min_data_in_leaf': 94, 'bagging_freq': 1, 'bagging_fraction': 0.3799269559001205, 'feature_fraction': 0.18115096173690304, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T78/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.06901887111456499, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 75, 'min_data_in_leaf': 40, 'bagging_freq': 1, 'bagging_fraction': 0.20659460707526944, 'feature_fraction': 0.8472753225073781, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T79/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.25563203042577176, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 48, 'min_data_in_leaf': 53, 'bagging_freq': 1, 'bagging_fraction': 0.06985901733934607, 'feature_fraction': 0.009688170796795847, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T80/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.08516202726680727, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 98, 'min_data_in_leaf': 97, 'bagging_freq': 1, 'bagging_fraction': 0.7791919742513885, 'feature_fraction': 0.9251634258375487, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T81/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.3885812205412932, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 61, 'min_data_in_leaf': 54, 'bagging_freq': 1, 'bagging_fraction': 0.3068100995451961, 'feature_fraction': 0.5775429488313755, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T82/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.002700700713465399, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 37, 'min_data_in_leaf': 27, 'bagging_freq': 1, 'bagging_fraction': 0.7836479651212873, 'feature_fraction': 0.5564294271917406, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.171894\n",
      "[2000]\tvalid_set's multi_error: 0.168409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T83/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0024933663413912073, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 64, 'min_data_in_leaf': 46, 'bagging_freq': 1, 'bagging_fraction': 0.5392235974137527, 'feature_fraction': 0.32568476641681543, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.173941\n",
      "[2000]\tvalid_set's multi_error: 0.169847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T84/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.017107644685186413, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 100, 'min_data_in_leaf': 97, 'bagging_freq': 1, 'bagging_fraction': 0.903983954928237, 'feature_fraction': 0.5438059500773263, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T85/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.011938709209716283, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 78, 'min_data_in_leaf': 32, 'bagging_freq': 1, 'bagging_fraction': 0.45860396176858587, 'feature_fraction': 0.7241676366115433, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.168235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T86/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.007664918362139111, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 63, 'min_data_in_leaf': 90, 'bagging_freq': 1, 'bagging_fraction': 0.6900250201912274, 'feature_fraction': 0.6996220542505167, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.167712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T87/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0027119986338300403, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 15, 'min_data_in_leaf': 85, 'bagging_freq': 1, 'bagging_fraction': 0.6360610554471413, 'feature_fraction': 0.24002027337970955, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.179169\n",
      "[2000]\tvalid_set's multi_error: 0.172809\n",
      "[3000]\tvalid_set's multi_error: 0.170021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T88/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.039359405970602106, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 43, 'min_data_in_leaf': 25, 'bagging_freq': 1, 'bagging_fraction': 0.9591666030352225, 'feature_fraction': 0.45813882726004285, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T89/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03580429414221614, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 43, 'min_data_in_leaf': 75, 'bagging_freq': 1, 'bagging_fraction': 0.45722345335385706, 'feature_fraction': 0.9518744768327362, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T90/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0026931147288920547, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 95, 'min_data_in_leaf': 41, 'bagging_freq': 1, 'bagging_fraction': 0.9088437184127384, 'feature_fraction': 0.8155238187685688, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.169324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T91/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.004990919179076706, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 98, 'min_data_in_leaf': 22, 'bagging_freq': 1, 'bagging_fraction': 0.0627129520233457, 'feature_fraction': 0.42403225188984195, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.171023\n",
      "[2000]\tvalid_set's multi_error: 0.167799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T92/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.009101853950900013, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 92, 'min_data_in_leaf': 20, 'bagging_freq': 1, 'bagging_fraction': 0.03330462654669619, 'feature_fraction': 0.9589827218634736, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.171458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T93/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.028014889211792477, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 36, 'min_data_in_leaf': 98, 'bagging_freq': 1, 'bagging_fraction': 0.5318491770716929, 'feature_fraction': 0.8453576818202398, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T94/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0017716571983676167, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 90, 'min_data_in_leaf': 93, 'bagging_freq': 1, 'bagging_fraction': 0.609177581558388, 'feature_fraction': 0.09847800336397083, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T95/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.18442522233508593, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 94, 'min_data_in_leaf': 76, 'bagging_freq': 1, 'bagging_fraction': 0.08653249019938802, 'feature_fraction': 0.23717329363871142, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T96/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.029500844112350745, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 56, 'min_data_in_leaf': 83, 'bagging_freq': 1, 'bagging_fraction': 0.5130748631715455, 'feature_fraction': 0.6498319735234533, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T97/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0010776690356166026, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 27, 'min_data_in_leaf': 18, 'bagging_freq': 1, 'bagging_fraction': 0.2531911937228519, 'feature_fraction': 0.13105523121525775, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T98/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.0027530183615319848, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 3, 'min_data_in_leaf': 47, 'bagging_freq': 1, 'bagging_fraction': 0.9903450015608939, 'feature_fraction': 0.4090540953730616, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.188752\n",
      "[2000]\tvalid_set's multi_error: 0.180476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T99/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_train.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/dataset_val.pkl\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.1459256771866733, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 80, 'min_data_in_leaf': 64, 'bagging_freq': 1, 'bagging_fraction': 0.3681024019815341, 'feature_fraction': 0.3127532957735243, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Saving AutogluonModels/ag-20240628_021311/models/LightGBM/T100/model.pkl\n",
      "Time for LightGBM model HPO: 1451.0054309368134\n",
      "Best hyperparameter configuration for LightGBM model: \n",
      "{'learning_rate': 0.029500844112350745, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 56, 'min_data_in_leaf': 83, 'bagging_freq': 1, 'bagging_fraction': 0.5130748631715455, 'feature_fraction': 0.6498319735234533, 'metric': 'multi_error', 'early_stopping_rounds': 400}\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T1/model.pkl\n",
      "Fitted model: LightGBM/T1 ...\n",
      "\t0.8337\t = Validation score   (accuracy)\n",
      "\t7.91s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T2/model.pkl\n",
      "Fitted model: LightGBM/T2 ...\n",
      "\t0.8317\t = Validation score   (accuracy)\n",
      "\t3.55s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T3/model.pkl\n",
      "Fitted model: LightGBM/T3 ...\n",
      "\t0.8339\t = Validation score   (accuracy)\n",
      "\t58.41s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T4/model.pkl\n",
      "Fitted model: LightGBM/T4 ...\n",
      "\t0.8281\t = Validation score   (accuracy)\n",
      "\t4.27s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T5/model.pkl\n",
      "Fitted model: LightGBM/T5 ...\n",
      "\t0.8333\t = Validation score   (accuracy)\n",
      "\t39.18s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T6/model.pkl\n",
      "Fitted model: LightGBM/T6 ...\n",
      "\t0.8312\t = Validation score   (accuracy)\n",
      "\t3.32s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T7/model.pkl\n",
      "Fitted model: LightGBM/T7 ...\n",
      "\t0.8338\t = Validation score   (accuracy)\n",
      "\t6.81s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T8/model.pkl\n",
      "Fitted model: LightGBM/T8 ...\n",
      "\t0.8328\t = Validation score   (accuracy)\n",
      "\t2.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T9/model.pkl\n",
      "Fitted model: LightGBM/T9 ...\n",
      "\t0.8277\t = Validation score   (accuracy)\n",
      "\t8.57s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T10/model.pkl\n",
      "Fitted model: LightGBM/T10 ...\n",
      "\t0.8257\t = Validation score   (accuracy)\n",
      "\t3.35s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T11/model.pkl\n",
      "Fitted model: LightGBM/T11 ...\n",
      "\t0.8336\t = Validation score   (accuracy)\n",
      "\t7.83s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T12/model.pkl\n",
      "Fitted model: LightGBM/T12 ...\n",
      "\t0.8284\t = Validation score   (accuracy)\n",
      "\t2.89s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T13/model.pkl\n",
      "Fitted model: LightGBM/T13 ...\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t4.27s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T14/model.pkl\n",
      "Fitted model: LightGBM/T14 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t2.12s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T15/model.pkl\n",
      "Fitted model: LightGBM/T15 ...\n",
      "\t0.8323\t = Validation score   (accuracy)\n",
      "\t9.77s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T16/model.pkl\n",
      "Fitted model: LightGBM/T16 ...\n",
      "\t0.8344\t = Validation score   (accuracy)\n",
      "\t35.21s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T17/model.pkl\n",
      "Fitted model: LightGBM/T17 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t1.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T18/model.pkl\n",
      "Fitted model: LightGBM/T18 ...\n",
      "\t0.8317\t = Validation score   (accuracy)\n",
      "\t7.36s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T19/model.pkl\n",
      "Fitted model: LightGBM/T19 ...\n",
      "\t0.8272\t = Validation score   (accuracy)\n",
      "\t55.86s\t = Training   runtime\n",
      "\t2.01s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T20/model.pkl\n",
      "Fitted model: LightGBM/T20 ...\n",
      "\t0.8295\t = Validation score   (accuracy)\n",
      "\t2.66s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T21/model.pkl\n",
      "Fitted model: LightGBM/T21 ...\n",
      "\t0.8326\t = Validation score   (accuracy)\n",
      "\t6.18s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T22/model.pkl\n",
      "Fitted model: LightGBM/T22 ...\n",
      "\t0.8282\t = Validation score   (accuracy)\n",
      "\t20.73s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T23/model.pkl\n",
      "Fitted model: LightGBM/T23 ...\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t26.43s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T24/model.pkl\n",
      "Fitted model: LightGBM/T24 ...\n",
      "\t0.8295\t = Validation score   (accuracy)\n",
      "\t6.54s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T25/model.pkl\n",
      "Fitted model: LightGBM/T25 ...\n",
      "\t0.8334\t = Validation score   (accuracy)\n",
      "\t6.3s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T26/model.pkl\n",
      "Fitted model: LightGBM/T26 ...\n",
      "\t0.8313\t = Validation score   (accuracy)\n",
      "\t2.76s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T27/model.pkl\n",
      "Fitted model: LightGBM/T27 ...\n",
      "\t0.8315\t = Validation score   (accuracy)\n",
      "\t3.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T28/model.pkl\n",
      "Fitted model: LightGBM/T28 ...\n",
      "\t0.8315\t = Validation score   (accuracy)\n",
      "\t2.28s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T29/model.pkl\n",
      "Fitted model: LightGBM/T29 ...\n",
      "\t0.8334\t = Validation score   (accuracy)\n",
      "\t5.93s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T30/model.pkl\n",
      "Fitted model: LightGBM/T30 ...\n",
      "\t0.8331\t = Validation score   (accuracy)\n",
      "\t9.11s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T31/model.pkl\n",
      "Fitted model: LightGBM/T31 ...\n",
      "\t0.8332\t = Validation score   (accuracy)\n",
      "\t33.15s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T32/model.pkl\n",
      "Fitted model: LightGBM/T32 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t2.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T33/model.pkl\n",
      "Fitted model: LightGBM/T33 ...\n",
      "\t0.8266\t = Validation score   (accuracy)\n",
      "\t9.61s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T34/model.pkl\n",
      "Fitted model: LightGBM/T34 ...\n",
      "\t0.828\t = Validation score   (accuracy)\n",
      "\t2.64s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T35/model.pkl\n",
      "Fitted model: LightGBM/T35 ...\n",
      "\t0.8257\t = Validation score   (accuracy)\n",
      "\t4.13s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T36/model.pkl\n",
      "Fitted model: LightGBM/T36 ...\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t2.82s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T37/model.pkl\n",
      "Fitted model: LightGBM/T37 ...\n",
      "\t0.8265\t = Validation score   (accuracy)\n",
      "\t4.56s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T38/model.pkl\n",
      "Fitted model: LightGBM/T38 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T39/model.pkl\n",
      "Fitted model: LightGBM/T39 ...\n",
      "\t0.832\t = Validation score   (accuracy)\n",
      "\t1.69s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T40/model.pkl\n",
      "Fitted model: LightGBM/T40 ...\n",
      "\t0.8277\t = Validation score   (accuracy)\n",
      "\t56.37s\t = Training   runtime\n",
      "\t1.28s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T41/model.pkl\n",
      "Fitted model: LightGBM/T41 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T42/model.pkl\n",
      "Fitted model: LightGBM/T42 ...\n",
      "\t0.8327\t = Validation score   (accuracy)\n",
      "\t53.07s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T43/model.pkl\n",
      "Fitted model: LightGBM/T43 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T44/model.pkl\n",
      "Fitted model: LightGBM/T44 ...\n",
      "\t0.8332\t = Validation score   (accuracy)\n",
      "\t8.37s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T45/model.pkl\n",
      "Fitted model: LightGBM/T45 ...\n",
      "\t0.8343\t = Validation score   (accuracy)\n",
      "\t44.47s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T46/model.pkl\n",
      "Fitted model: LightGBM/T46 ...\n",
      "\t0.834\t = Validation score   (accuracy)\n",
      "\t14.44s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T47/model.pkl\n",
      "Fitted model: LightGBM/T47 ...\n",
      "\t0.8283\t = Validation score   (accuracy)\n",
      "\t18.18s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T48/model.pkl\n",
      "Fitted model: LightGBM/T48 ...\n",
      "\t0.8326\t = Validation score   (accuracy)\n",
      "\t5.4s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T49/model.pkl\n",
      "Fitted model: LightGBM/T49 ...\n",
      "\t0.8335\t = Validation score   (accuracy)\n",
      "\t7.23s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T50/model.pkl\n",
      "Fitted model: LightGBM/T50 ...\n",
      "\t0.8312\t = Validation score   (accuracy)\n",
      "\t10.72s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T51/model.pkl\n",
      "Fitted model: LightGBM/T51 ...\n",
      "\t0.8254\t = Validation score   (accuracy)\n",
      "\t2.38s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T52/model.pkl\n",
      "Fitted model: LightGBM/T52 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t1.93s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T53/model.pkl\n",
      "Fitted model: LightGBM/T53 ...\n",
      "\t0.8323\t = Validation score   (accuracy)\n",
      "\t4.41s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T54/model.pkl\n",
      "Fitted model: LightGBM/T54 ...\n",
      "\t0.8323\t = Validation score   (accuracy)\n",
      "\t49.38s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T55/model.pkl\n",
      "Fitted model: LightGBM/T55 ...\n",
      "\t0.8329\t = Validation score   (accuracy)\n",
      "\t9.64s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T56/model.pkl\n",
      "Fitted model: LightGBM/T56 ...\n",
      "\t0.834\t = Validation score   (accuracy)\n",
      "\t18.7s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T57/model.pkl\n",
      "Fitted model: LightGBM/T57 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T58/model.pkl\n",
      "Fitted model: LightGBM/T58 ...\n",
      "\t0.8318\t = Validation score   (accuracy)\n",
      "\t10.74s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T59/model.pkl\n",
      "Fitted model: LightGBM/T59 ...\n",
      "\t0.8337\t = Validation score   (accuracy)\n",
      "\t25.56s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T60/model.pkl\n",
      "Fitted model: LightGBM/T60 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t2.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T61/model.pkl\n",
      "Fitted model: LightGBM/T61 ...\n",
      "\t0.8298\t = Validation score   (accuracy)\n",
      "\t33.38s\t = Training   runtime\n",
      "\t0.83s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T62/model.pkl\n",
      "Fitted model: LightGBM/T62 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T63/model.pkl\n",
      "Fitted model: LightGBM/T63 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t0.93s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T64/model.pkl\n",
      "Fitted model: LightGBM/T64 ...\n",
      "\t0.833\t = Validation score   (accuracy)\n",
      "\t62.62s\t = Training   runtime\n",
      "\t0.79s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T65/model.pkl\n",
      "Fitted model: LightGBM/T65 ...\n",
      "\t0.8329\t = Validation score   (accuracy)\n",
      "\t4.98s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T66/model.pkl\n",
      "Fitted model: LightGBM/T66 ...\n",
      "\t0.8331\t = Validation score   (accuracy)\n",
      "\t2.35s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T67/model.pkl\n",
      "Fitted model: LightGBM/T67 ...\n",
      "\t0.8222\t = Validation score   (accuracy)\n",
      "\t15.0s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T68/model.pkl\n",
      "Fitted model: LightGBM/T68 ...\n",
      "\t0.8313\t = Validation score   (accuracy)\n",
      "\t6.3s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T69/model.pkl\n",
      "Fitted model: LightGBM/T69 ...\n",
      "\t0.8318\t = Validation score   (accuracy)\n",
      "\t31.86s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T70/model.pkl\n",
      "Fitted model: LightGBM/T70 ...\n",
      "\t0.8319\t = Validation score   (accuracy)\n",
      "\t4.96s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T71/model.pkl\n",
      "Fitted model: LightGBM/T71 ...\n",
      "\t0.8325\t = Validation score   (accuracy)\n",
      "\t17.04s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T72/model.pkl\n",
      "Fitted model: LightGBM/T72 ...\n",
      "\t0.8263\t = Validation score   (accuracy)\n",
      "\t2.21s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T73/model.pkl\n",
      "Fitted model: LightGBM/T73 ...\n",
      "\t0.8285\t = Validation score   (accuracy)\n",
      "\t18.66s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T74/model.pkl\n",
      "Fitted model: LightGBM/T74 ...\n",
      "\t0.8334\t = Validation score   (accuracy)\n",
      "\t8.17s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T75/model.pkl\n",
      "Fitted model: LightGBM/T75 ...\n",
      "\t0.8311\t = Validation score   (accuracy)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T76/model.pkl\n",
      "Fitted model: LightGBM/T76 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t1.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T77/model.pkl\n",
      "Fitted model: LightGBM/T77 ...\n",
      "\t0.8332\t = Validation score   (accuracy)\n",
      "\t19.76s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T78/model.pkl\n",
      "Fitted model: LightGBM/T78 ...\n",
      "\t0.8313\t = Validation score   (accuracy)\n",
      "\t4.08s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T79/model.pkl\n",
      "Fitted model: LightGBM/T79 ...\n",
      "\t0.8301\t = Validation score   (accuracy)\n",
      "\t3.28s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T80/model.pkl\n",
      "Fitted model: LightGBM/T80 ...\n",
      "\t0.8273\t = Validation score   (accuracy)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T81/model.pkl\n",
      "Fitted model: LightGBM/T81 ...\n",
      "\t0.8317\t = Validation score   (accuracy)\n",
      "\t4.91s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T82/model.pkl\n",
      "Fitted model: LightGBM/T82 ...\n",
      "\t0.8233\t = Validation score   (accuracy)\n",
      "\t1.89s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T83/model.pkl\n",
      "Fitted model: LightGBM/T83 ...\n",
      "\t0.8321\t = Validation score   (accuracy)\n",
      "\t41.98s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T84/model.pkl\n",
      "Fitted model: LightGBM/T84 ...\n",
      "\t0.8305\t = Validation score   (accuracy)\n",
      "\t50.18s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T85/model.pkl\n",
      "Fitted model: LightGBM/T85 ...\n",
      "\t0.8343\t = Validation score   (accuracy)\n",
      "\t21.57s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T86/model.pkl\n",
      "Fitted model: LightGBM/T86 ...\n",
      "\t0.833\t = Validation score   (accuracy)\n",
      "\t28.65s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T87/model.pkl\n",
      "Fitted model: LightGBM/T87 ...\n",
      "\t0.8333\t = Validation score   (accuracy)\n",
      "\t43.7s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T88/model.pkl\n",
      "Fitted model: LightGBM/T88 ...\n",
      "\t0.8313\t = Validation score   (accuracy)\n",
      "\t33.91s\t = Training   runtime\n",
      "\t0.63s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T89/model.pkl\n",
      "Fitted model: LightGBM/T89 ...\n",
      "\t0.832\t = Validation score   (accuracy)\n",
      "\t4.82s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T90/model.pkl\n",
      "Fitted model: LightGBM/T90 ...\n",
      "\t0.8319\t = Validation score   (accuracy)\n",
      "\t4.41s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T91/model.pkl\n",
      "Fitted model: LightGBM/T91 ...\n",
      "\t0.8307\t = Validation score   (accuracy)\n",
      "\t43.35s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T92/model.pkl\n",
      "Fitted model: LightGBM/T92 ...\n",
      "\t0.8325\t = Validation score   (accuracy)\n",
      "\t60.41s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T93/model.pkl\n",
      "Fitted model: LightGBM/T93 ...\n",
      "\t0.8287\t = Validation score   (accuracy)\n",
      "\t24.21s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T94/model.pkl\n",
      "Fitted model: LightGBM/T94 ...\n",
      "\t0.8336\t = Validation score   (accuracy)\n",
      "\t9.9s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T95/model.pkl\n",
      "Fitted model: LightGBM/T95 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t2.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T96/model.pkl\n",
      "Fitted model: LightGBM/T96 ...\n",
      "\t0.8254\t = Validation score   (accuracy)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T97/model.pkl\n",
      "Fitted model: LightGBM/T97 ...\n",
      "\t0.8345\t = Validation score   (accuracy)\n",
      "\t9.96s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T98/model.pkl\n",
      "Fitted model: LightGBM/T98 ...\n",
      "\t0.4742\t = Validation score   (accuracy)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T99/model.pkl\n",
      "Fitted model: LightGBM/T99 ...\n",
      "\t0.8211\t = Validation score   (accuracy)\n",
      "\t13.55s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T100/model.pkl\n",
      "Fitted model: LightGBM/T100 ...\n",
      "\t0.8297\t = Validation score   (accuracy)\n",
      "\t4.88s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Saving AutogluonModels/ag-20240628_021311/models/trainer.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T3/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T4/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T5/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T6/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T7/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T8/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T9/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T10/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T11/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T12/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T13/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T14/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T15/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T16/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T17/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T18/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T19/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T20/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T21/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T22/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T23/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T24/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T25/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T26/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T27/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T28/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T29/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T30/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T31/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T32/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T33/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T34/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T35/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T36/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T37/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T38/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T39/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T40/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T41/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T42/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T43/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T44/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T45/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T46/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T47/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T48/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T49/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T50/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T51/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T52/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T53/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T54/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T55/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T56/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T57/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T58/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T59/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T60/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T61/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T62/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T63/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T64/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T65/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T66/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T67/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T68/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T69/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T70/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T71/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T72/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T73/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T74/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T75/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T76/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T77/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T78/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T79/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T80/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T81/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T82/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T83/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T84/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T85/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T86/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T87/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T88/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T89/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T90/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T91/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T92/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T93/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T94/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T95/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T96/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T97/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T98/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T99/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T100/model.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 64\n",
      "Saving AutogluonModels/ag-20240628_021311/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Ensemble size: 1\n",
      "Ensemble weights: \n",
      "[0. 0. 0. 0. 1.]\n",
      "Saving AutogluonModels/ag-20240628_021311/models/WeightedEnsemble_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/models/WeightedEnsemble_L2/model.pkl\n",
      "\tEnsemble Weights: {'LightGBM/T97': 1.0}\n",
      "\t0.8345\t = Validation score   (accuracy)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving AutogluonModels/ag-20240628_021311/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/models/trainer.pkl\n",
      "AutoGluon training complete, total runtime = 1485.88s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/learner.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/predictor.pkl\n",
      "Saving AutogluonModels/ag-20240628_021311/__version__ with contents \"1.1.0\"\n",
      "Saving AutogluonModels/ag-20240628_021311/metadata.json\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240628_021311\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label='Target',\n",
    "    problem_type='multiclass',\n",
    "    eval_metric='accuracy',\n",
    "    verbosity=3,\n",
    ").fit(\n",
    "    train_df, \n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    "    num_stack_levels=0,\n",
    "    holdout_frac=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1c360592-9b7a-4278-b860-e95dd8c9ea08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T1/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T1/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T2/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T3/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T3/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T4/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T4/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T5/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T5/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T6/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T6/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T7/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T7/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T8/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T8/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T9/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T9/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T10/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T10/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T11/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T11/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T12/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T12/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T13/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T13/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T14/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T14/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T15/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T15/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T16/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T16/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T17/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T17/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T18/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T18/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T19/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T19/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T20/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T20/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T21/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T21/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T22/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T22/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T23/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T23/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T24/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T24/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T25/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T25/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T26/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T26/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T27/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T27/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T28/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T28/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T29/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T29/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T30/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T30/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T31/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T31/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T32/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T32/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T33/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T33/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T34/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T34/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T35/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T35/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T36/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T36/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T37/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T37/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T38/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T38/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T39/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T39/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T40/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T40/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T41/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T41/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T42/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T42/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T43/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T43/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T44/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T44/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T45/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T45/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T46/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T46/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T47/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T47/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T48/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T48/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T49/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T49/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T50/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T50/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T51/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T51/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T52/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T52/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T53/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T53/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T54/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T54/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T55/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T55/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T56/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T56/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T57/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T57/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T58/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T58/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T59/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T59/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T60/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T60/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T61/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T61/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T62/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T62/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T63/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T63/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T64/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T64/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T65/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T65/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T66/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T66/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T67/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T67/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T68/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T68/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T69/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T69/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T70/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T70/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T71/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T71/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T72/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T72/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T73/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T73/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T74/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T74/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T75/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T75/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T76/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T76/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T77/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T77/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T78/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T78/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T79/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T79/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T80/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T80/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T81/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T81/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T82/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T82/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T83/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T83/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T84/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T84/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T85/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T85/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T86/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T86/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T87/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T87/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T88/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T88/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T89/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T89/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T90/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T90/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T91/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T91/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T92/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T92/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T93/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T93/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T94/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T94/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T95/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T95/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T96/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T96/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T97/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T97/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T98/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T98/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T99/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T99/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T100/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/LightGBM/T100/model.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/WeightedEnsemble_L2/info.pkl\n",
      "Loading: AutogluonModels/ag-20240628_021311/models/WeightedEnsemble_L2/model.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "      <th>...</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>hyperparameters_fit</th>\n",
       "      <th>ag_args_fit</th>\n",
       "      <th>features</th>\n",
       "      <th>compile_time</th>\n",
       "      <th>child_hyperparameters</th>\n",
       "      <th>child_hyperparameters_fit</th>\n",
       "      <th>child_ag_args_fit</th>\n",
       "      <th>ancestors</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM/T97</td>\n",
       "      <td>0.834509</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.127338</td>\n",
       "      <td>9.959136</td>\n",
       "      <td>0.127338</td>\n",
       "      <td>9.959136</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>{'learning_rate': 0.029500844112350745, 'objec...</td>\n",
       "      <td>{'num_boost_round': 321}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Daytime_evening_attendance, Previous_qualific...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.834509</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.130377</td>\n",
       "      <td>10.366061</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.406924</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>{'use_orig_features': False, 'max_base_models'...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBM/T97_2, LightGBM/T97_1, LightGBM/T97_0]</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ensemble_size': 25, 'subsample_size': 1000000}</td>\n",
       "      <td>{'ensemble_size': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBM/T97]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM/T16</td>\n",
       "      <td>0.834422</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.523794</td>\n",
       "      <td>35.213979</td>\n",
       "      <td>0.523794</td>\n",
       "      <td>35.213979</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>{'learning_rate': 0.0071798932488217025, 'obje...</td>\n",
       "      <td>{'num_boost_round': 1657}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Daytime_evening_attendance, Previous_qualific...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM/T85</td>\n",
       "      <td>0.834292</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.204002</td>\n",
       "      <td>21.573351</td>\n",
       "      <td>0.204002</td>\n",
       "      <td>21.573351</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>{'learning_rate': 0.017107644685186413, 'objec...</td>\n",
       "      <td>{'num_boost_round': 465}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Daytime_evening_attendance, Previous_qualific...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM/T45</td>\n",
       "      <td>0.834292</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.542179</td>\n",
       "      <td>44.468923</td>\n",
       "      <td>0.542179</td>\n",
       "      <td>44.468923</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>{'learning_rate': 0.011879186536249427, 'objec...</td>\n",
       "      <td>{'num_boost_round': 1423}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Daytime_evening_attendance, Previous_qualific...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>LightGBM/T38</td>\n",
       "      <td>0.474168</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.016324</td>\n",
       "      <td>1.315640</td>\n",
       "      <td>0.016324</td>\n",
       "      <td>1.315640</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>{'learning_rate': 0.0015370204507549887, 'obje...</td>\n",
       "      <td>{'num_boost_round': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Daytime_evening_attendance, Previous_qualific...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>LightGBM/T57</td>\n",
       "      <td>0.474168</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.016430</td>\n",
       "      <td>2.068895</td>\n",
       "      <td>0.016430</td>\n",
       "      <td>2.068895</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>{'learning_rate': 0.002075940748858884, 'objec...</td>\n",
       "      <td>{'num_boost_round': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Daytime_evening_attendance, Previous_qualific...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>LightGBM/T14</td>\n",
       "      <td>0.474168</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.016719</td>\n",
       "      <td>2.116223</td>\n",
       "      <td>0.016719</td>\n",
       "      <td>2.116223</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>{'learning_rate': 0.0018854537484233854, 'obje...</td>\n",
       "      <td>{'num_boost_round': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Daytime_evening_attendance, Previous_qualific...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>LightGBM/T17</td>\n",
       "      <td>0.474168</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.017531</td>\n",
       "      <td>1.585774</td>\n",
       "      <td>0.017531</td>\n",
       "      <td>1.585774</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>{'learning_rate': 0.0010880013579881585, 'obje...</td>\n",
       "      <td>{'num_boost_round': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Daytime_evening_attendance, Previous_qualific...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>LightGBM/T62</td>\n",
       "      <td>0.474168</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.020288</td>\n",
       "      <td>0.878821</td>\n",
       "      <td>0.020288</td>\n",
       "      <td>0.878821</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>{'learning_rate': 0.0010591041777891583, 'obje...</td>\n",
       "      <td>{'num_boost_round': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[Daytime_evening_attendance, Previous_qualific...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  score_val eval_metric  pred_time_val   fit_time  \\\n",
       "0           LightGBM/T97   0.834509    accuracy       0.127338   9.959136   \n",
       "1    WeightedEnsemble_L2   0.834509    accuracy       0.130377  10.366061   \n",
       "2           LightGBM/T16   0.834422    accuracy       0.523794  35.213979   \n",
       "3           LightGBM/T85   0.834292    accuracy       0.204002  21.573351   \n",
       "4           LightGBM/T45   0.834292    accuracy       0.542179  44.468923   \n",
       "..                   ...        ...         ...            ...        ...   \n",
       "96          LightGBM/T38   0.474168    accuracy       0.016324   1.315640   \n",
       "97          LightGBM/T57   0.474168    accuracy       0.016430   2.068895   \n",
       "98          LightGBM/T14   0.474168    accuracy       0.016719   2.116223   \n",
       "99          LightGBM/T17   0.474168    accuracy       0.017531   1.585774   \n",
       "100         LightGBM/T62   0.474168    accuracy       0.020288   0.878821   \n",
       "\n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                  0.127338           9.959136            1       True   \n",
       "1                  0.003039           0.406924            2       True   \n",
       "2                  0.523794          35.213979            1       True   \n",
       "3                  0.204002          21.573351            1       True   \n",
       "4                  0.542179          44.468923            1       True   \n",
       "..                      ...                ...          ...        ...   \n",
       "96                 0.016324           1.315640            1       True   \n",
       "97                 0.016430           2.068895            1       True   \n",
       "98                 0.016719           2.116223            1       True   \n",
       "99                 0.017531           1.585774            1       True   \n",
       "100                0.020288           0.878821            1       True   \n",
       "\n",
       "     fit_order  ...                                    hyperparameters  \\\n",
       "0           97  ...  {'learning_rate': 0.029500844112350745, 'objec...   \n",
       "1          101  ...  {'use_orig_features': False, 'max_base_models'...   \n",
       "2           16  ...  {'learning_rate': 0.0071798932488217025, 'obje...   \n",
       "3           85  ...  {'learning_rate': 0.017107644685186413, 'objec...   \n",
       "4           45  ...  {'learning_rate': 0.011879186536249427, 'objec...   \n",
       "..         ...  ...                                                ...   \n",
       "96          38  ...  {'learning_rate': 0.0015370204507549887, 'obje...   \n",
       "97          57  ...  {'learning_rate': 0.002075940748858884, 'objec...   \n",
       "98          14  ...  {'learning_rate': 0.0018854537484233854, 'obje...   \n",
       "99          17  ...  {'learning_rate': 0.0010880013579881585, 'obje...   \n",
       "100         62  ...  {'learning_rate': 0.0010591041777891583, 'obje...   \n",
       "\n",
       "           hyperparameters_fit  \\\n",
       "0     {'num_boost_round': 321}   \n",
       "1                           {}   \n",
       "2    {'num_boost_round': 1657}   \n",
       "3     {'num_boost_round': 465}   \n",
       "4    {'num_boost_round': 1423}   \n",
       "..                         ...   \n",
       "96      {'num_boost_round': 1}   \n",
       "97      {'num_boost_round': 1}   \n",
       "98      {'num_boost_round': 1}   \n",
       "99      {'num_boost_round': 1}   \n",
       "100     {'num_boost_round': 1}   \n",
       "\n",
       "                                           ag_args_fit  \\\n",
       "0    {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "1    {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "2    {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "3    {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "4    {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "..                                                 ...   \n",
       "96   {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "97   {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "98   {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "99   {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "100  {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "\n",
       "                                              features  compile_time  \\\n",
       "0    [Daytime_evening_attendance, Previous_qualific...          None   \n",
       "1     [LightGBM/T97_2, LightGBM/T97_1, LightGBM/T97_0]          None   \n",
       "2    [Daytime_evening_attendance, Previous_qualific...          None   \n",
       "3    [Daytime_evening_attendance, Previous_qualific...          None   \n",
       "4    [Daytime_evening_attendance, Previous_qualific...          None   \n",
       "..                                                 ...           ...   \n",
       "96   [Daytime_evening_attendance, Previous_qualific...          None   \n",
       "97   [Daytime_evening_attendance, Previous_qualific...          None   \n",
       "98   [Daytime_evening_attendance, Previous_qualific...          None   \n",
       "99   [Daytime_evening_attendance, Previous_qualific...          None   \n",
       "100  [Daytime_evening_attendance, Previous_qualific...          None   \n",
       "\n",
       "                                child_hyperparameters  \\\n",
       "0                                                None   \n",
       "1    {'ensemble_size': 25, 'subsample_size': 1000000}   \n",
       "2                                                None   \n",
       "3                                                None   \n",
       "4                                                None   \n",
       "..                                                ...   \n",
       "96                                               None   \n",
       "97                                               None   \n",
       "98                                               None   \n",
       "99                                               None   \n",
       "100                                              None   \n",
       "\n",
       "     child_hyperparameters_fit  \\\n",
       "0                         None   \n",
       "1         {'ensemble_size': 1}   \n",
       "2                         None   \n",
       "3                         None   \n",
       "4                         None   \n",
       "..                         ...   \n",
       "96                        None   \n",
       "97                        None   \n",
       "98                        None   \n",
       "99                        None   \n",
       "100                       None   \n",
       "\n",
       "                                     child_ag_args_fit       ancestors  \\\n",
       "0                                                 None              []   \n",
       "1    {'max_memory_usage_ratio': 1.0, 'max_time_limi...  [LightGBM/T97]   \n",
       "2                                                 None              []   \n",
       "3                                                 None              []   \n",
       "4                                                 None              []   \n",
       "..                                                 ...             ...   \n",
       "96                                                None              []   \n",
       "97                                                None              []   \n",
       "98                                                None              []   \n",
       "99                                                None              []   \n",
       "100                                               None              []   \n",
       "\n",
       "               descendants  \n",
       "0    [WeightedEnsemble_L2]  \n",
       "1                       []  \n",
       "2                       []  \n",
       "3                       []  \n",
       "4                       []  \n",
       "..                     ...  \n",
       "96                      []  \n",
       "97                      []  \n",
       "98                      []  \n",
       "99                      []  \n",
       "100                     []  \n",
       "\n",
       "[101 rows x 32 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_df = predictor.leaderboard(extra_info=True)\n",
    "lb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6bc9901e-0b05-4dc2-9e39-074f4637213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.029500844112350745,\n",
       " 'objective': 'multiclass',\n",
       " 'num_class': 3,\n",
       " 'num_leaves': 56,\n",
       " 'min_data_in_leaf': 83,\n",
       " 'bagging_freq': 1,\n",
       " 'bagging_fraction': 0.5130748631715455,\n",
       " 'feature_fraction': 0.6498319735234533,\n",
       " 'metric': 'multi_error',\n",
       " 'early_stopping_rounds': 400}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_df.iloc[0].hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c5b7f306-df4c-4d64-88fd-0d8acfc8007e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_boost_round': 321}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_df.iloc[0].hyperparameters_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f8362-7b08-4317-b1c8-5ff0d610cc86",
   "metadata": {},
   "source": [
    "#### 搜索到的参数在5-fold交叉验证上的指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2c5a865e-1f66-4443-a0f7-76dba5e6e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'GBM': lb_df.iloc[0].hyperparameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eaa6972f-229a-4820-bb65-d2f71d0398bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GBM': {'learning_rate': 0.029500844112350745,\n",
       "  'objective': 'multiclass',\n",
       "  'num_class': 3,\n",
       "  'num_leaves': 56,\n",
       "  'min_data_in_leaf': 83,\n",
       "  'bagging_freq': 1,\n",
       "  'bagging_fraction': 0.5130748631715455,\n",
       "  'feature_fraction': 0.6498319735234533,\n",
       "  'metric': 'multi_error',\n",
       "  'early_stopping_rounds': 400}}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e10c2226-ddab-4418-b41b-4e6d1d1e0a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240628_061312\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240628_061312\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Sun Jan 19 18:21:42 CST 2020\n",
      "CPU Count:          64\n",
      "Memory Avail:       92.97 GB / 251.62 GB (36.9%)\n",
      "Disk Space Avail:   105.84 GB / 879.22 GB (12.0%)\n",
      "===================================================\n",
      "Train Data Rows:    76518\n",
      "Train Data Columns: 36\n",
      "Label Column:       Target\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    95210.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11.82 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t('float', [])    :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])      : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t36 features in original data used to generate 36 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 11.83 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': {'learning_rate': 0.029500844112350745, 'objective': 'multiclass', 'num_class': 3, 'num_leaves': 56, 'min_data_in_leaf': 83, 'bagging_freq': 1, 'bagging_fraction': 0.5130748631715455, 'feature_fraction': 0.6498319735234533, 'metric': 'multi_error', 'early_stopping_rounds': 400},\n",
      "}\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=12, gpus=0, memory=0.10%)\n",
      "\t0.8333\t = Validation score   (accuracy)\n",
      "\t9.92s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.8333\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 11.85s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240628_061312\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label='Target',\n",
    "    problem_type='multiclass',\n",
    "    eval_metric='accuracy',\n",
    "    verbosity=2,\n",
    ").fit(\n",
    "    train_df, \n",
    "    hyperparameters=hyperparameters,\n",
    "    num_bag_sets=1,\n",
    "    num_bag_folds=5,\n",
    "    num_stack_levels=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0b8e453a-9670-4c7e-bdfc-bb4245ab7706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.833294</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.999749</td>\n",
       "      <td>9.922045</td>\n",
       "      <td>0.999749</td>\n",
       "      <td>9.922045</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.833294</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.007860</td>\n",
       "      <td>9.934711</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_val eval_metric  pred_time_val  fit_time  \\\n",
       "0      LightGBM_BAG_L1   0.833294    accuracy       0.999749  9.922045   \n",
       "1  WeightedEnsemble_L2   0.833294    accuracy       1.007860  9.934711   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.999749           9.922045            1       True   \n",
       "1                0.008111           0.012666            2       True   \n",
       "\n",
       "   fit_order  \n",
       "0          1  \n",
       "1          2  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b2225-741f-410e-9ec4-99f47fa753d5",
   "metadata": {},
   "source": [
    "### 神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dca5ed5c-665d-4d81-bd57-7d40fba08c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240628_084914\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtuning_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtime_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpresets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'List[str] | str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhyperparameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dict | str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeature_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'infer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minfer_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minfer_limit_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_weighted_ensemble\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_full_last_level_weighted_ensemble\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfull_weighted_ensemble_additionally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdynamic_stacking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcalibrate_decision_threshold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_cpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Fit models to predict a column of a data table (label) based on the other columns (features).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "train_data : str or :class:`TabularDataset` or :class:`pd.DataFrame`\n",
       "    Table of the training data, which is similar to a pandas DataFrame.\n",
       "    If str is passed, `train_data` will be loaded using the str value as the file path.\n",
       "tuning_data : str or :class:`TabularDataset` or :class:`pd.DataFrame`, default = None\n",
       "    Another dataset containing validation data reserved for tuning processes such as early stopping and hyperparameter tuning.\n",
       "    This dataset should be in the same format as `train_data`.\n",
       "    If str is passed, `tuning_data` will be loaded using the str value as the file path.\n",
       "    Note: final model returned may be fit on `tuning_data` as well as `train_data`. Do not provide your evaluation test data here!\n",
       "    In particular, when `num_bag_folds` > 0 or `num_stack_levels` > 0, models will be trained on both `tuning_data` and `train_data`.\n",
       "    If `tuning_data = None`, `fit()` will automatically hold out some random validation examples from `train_data`.\n",
       "time_limit : int, default = None\n",
       "    Approximately how long `fit()` should run for (wallclock time in seconds).\n",
       "    If not specified, `fit()` will run until all models have completed training, but will not repeatedly bag models unless `num_bag_sets` is specified.\n",
       "presets : list or str or dict, default = ['medium_quality']\n",
       "    List of preset configurations for various arguments in `fit()`. Can significantly impact predictive accuracy, memory-footprint, and inference latency of trained models, and various other properties of the returned `predictor`.\n",
       "    It is recommended to specify presets and avoid specifying most other `fit()` arguments or model hyperparameters prior to becoming familiar with AutoGluon.\n",
       "    As an example, to get the most accurate overall predictor (regardless of its efficiency), set `presets='best_quality'`.\n",
       "    To get good quality with minimal disk usage, set `presets=['good_quality', 'optimize_for_deployment']`\n",
       "    Any user-specified arguments in `fit()` will override the values used by presets.\n",
       "    If specifying a list of presets, later presets will override earlier presets if they alter the same argument.\n",
       "    For precise definitions of the provided presets, see file: `autogluon/tabular/configs/presets_configs.py`.\n",
       "    Users can specify custom presets by passing in a dictionary of argument values as an element to the list.\n",
       "\n",
       "    Available Presets: ['best_quality', 'high_quality', 'good_quality', 'medium_quality', 'optimize_for_deployment', 'interpretable', 'ignore_text']\n",
       "\n",
       "    It is recommended to only use one `quality` based preset in a given call to `fit()` as they alter many of the same arguments and are not compatible with each-other.\n",
       "\n",
       "    In-depth Preset Info:\n",
       "        best_quality={'auto_stack': True}\n",
       "            Best predictive accuracy with little consideration to inference time or disk usage. Achieve even better results by specifying a large time_limit value.\n",
       "            Recommended for applications that benefit from the best possible model accuracy.\n",
       "\n",
       "        high_quality={'auto_stack': True, 'refit_full': True, 'set_best_to_refit_full': True, 'save_bag_folds': False}\n",
       "            High predictive accuracy with fast inference. ~10x-200x faster inference and ~10x-200x lower disk usage than `best_quality`.\n",
       "            Recommended for applications that require reasonable inference speed and/or model size.\n",
       "\n",
       "        good_quality={'auto_stack': True, 'refit_full': True, 'set_best_to_refit_full': True, 'save_bag_folds': False, 'hyperparameters': 'light'}\n",
       "            Good predictive accuracy with very fast inference. ~4x faster inference and ~4x lower disk usage than `high_quality`.\n",
       "            Recommended for applications that require fast inference speed.\n",
       "\n",
       "        medium_quality={'auto_stack': False}\n",
       "            Medium predictive accuracy with very fast inference and very fast training time. ~20x faster training than `good_quality`.\n",
       "            This is the default preset in AutoGluon, but should generally only be used for quick prototyping, as `good_quality` results in significantly better predictive accuracy and faster inference time.\n",
       "\n",
       "        optimize_for_deployment={'keep_only_best': True, 'save_space': True}\n",
       "            Optimizes result immediately for deployment by deleting unused models and removing training artifacts.\n",
       "            Often can reduce disk usage by ~2-4x with no negatives to model accuracy or inference speed.\n",
       "            This will disable numerous advanced functionality, but has no impact on inference.\n",
       "            This will make certain functionality less informative, such as `predictor.leaderboard()` and `predictor.fit_summary()`.\n",
       "                Because unused models will be deleted under this preset, methods like `predictor.leaderboard()` and `predictor.fit_summary()` will no longer show the full set of models that were trained during `fit()`.\n",
       "            Recommended for applications where the inner details of AutoGluon's training is not important and there is no intention of manually choosing between the final models.\n",
       "            This preset pairs well with the other presets such as `good_quality` to make a very compact final model.\n",
       "            Identical to calling `predictor.delete_models(models_to_keep='best', dry_run=False)` and `predictor.save_space()` directly after `fit()`.\n",
       "\n",
       "        interpretable={'auto_stack': False, 'hyperparameters': 'interpretable'}\n",
       "            Fits only interpretable rule-based models from the imodels package.\n",
       "            Trades off predictive accuracy for conciseness.\n",
       "\n",
       "        ignore_text={'_feature_generator_kwargs': {'enable_text_ngram_features': False, 'enable_text_special_features': False, 'enable_raw_text_features': False}}\n",
       "            Disables automated feature generation when text features are detected.\n",
       "            This is useful to determine how beneficial text features are to the end result, as well as to ensure features are not mistaken for text when they are not.\n",
       "            Ignored if `feature_generator` was also specified.\n",
       "\n",
       "hyperparameters : str or dict, default = 'default'\n",
       "    Determines the hyperparameters used by the models.\n",
       "    If `str` is passed, will use a preset hyperparameter configuration.\n",
       "        Valid `str` options: ['default', 'light', 'very_light', 'toy', 'multimodal']\n",
       "            'default': Default AutoGluon hyperparameters intended to maximize accuracy without significant regard to inference time or disk usage.\n",
       "            'light': Results in smaller models. Generally will make inference speed much faster and disk usage much lower, but with worse accuracy.\n",
       "            'very_light': Results in much smaller models. Behaves similarly to 'light', but in many cases with over 10x less disk usage and a further reduction in accuracy.\n",
       "            'toy': Results in extremely small models. Only use this when prototyping, as the model quality will be severely reduced.\n",
       "            'multimodal': [EXPERIMENTAL] Trains a multimodal transformer model alongside tabular models. Requires that some text columns appear in the data and GPU.\n",
       "                When combined with 'best_quality' `presets` option, this can achieve extremely strong results in multimodal data tables that contain columns with text in addition to numeric/categorical columns.\n",
       "        Reference `autogluon/tabular/configs/hyperparameter_configs.py` for information on the hyperparameters associated with each preset.\n",
       "    Keys are strings that indicate which model types to train.\n",
       "        Stable model options include:\n",
       "            'GBM' (LightGBM)\n",
       "            'CAT' (CatBoost)\n",
       "            'XGB' (XGBoost)\n",
       "            'RF' (random forest)\n",
       "            'XT' (extremely randomized trees)\n",
       "            'KNN' (k-nearest neighbors)\n",
       "            'LR' (linear regression)\n",
       "            'NN_TORCH' (neural network implemented in Pytorch)\n",
       "            'FASTAI' (neural network with FastAI backend)\n",
       "            'AG_AUTOMM' (`MultimodalPredictor` from `autogluon.multimodal`. Supports Tabular, Text, and Image modalities. GPU is required.)\n",
       "        Experimental model options include:\n",
       "            'FT_TRANSFORMER' (Tabular Transformer, GPU is recommended. Does not scale well to >100 features.)\n",
       "            'FASTTEXT' (FastText)\n",
       "            'VW' (VowpalWabbit)\n",
       "            'AG_TEXT_NN' (Multimodal Text+Tabular model, GPU is required. Recommended to instead use its successor, 'AG_AUTOMM'.)\n",
       "            'AG_IMAGE_NN' (Image model, GPU is required. Recommended to instead use its successor, 'AG_AUTOMM'.)\n",
       "        If a certain key is missing from hyperparameters, then `fit()` will not train any models of that type. Omitting a model key from hyperparameters is equivalent to including this model key in `excluded_model_types`.\n",
       "        For example, set `hyperparameters = { 'NN_TORCH':{...} }` if say you only want to train (PyTorch) neural networks and no other types of models.\n",
       "    Values = dict of hyperparameter settings for each model type, or list of dicts.\n",
       "        Each hyperparameter can either be a single fixed value or a search space containing many possible values.\n",
       "        Unspecified hyperparameters will be set to default values (or default search spaces if `hyperparameter_tune_kwargs='auto'`).\n",
       "        Caution: Any provided search spaces will error if `hyperparameter_tune_kwargs=None` (Default).\n",
       "        To train multiple models of a given type, set the value to a list of hyperparameter dictionaries.\n",
       "            For example, `hyperparameters = {'RF': [{'criterion': 'gini'}, {'criterion': 'entropy'}]}` will result in 2 random forest models being trained with separate hyperparameters.\n",
       "        Some model types have preset hyperparameter configs keyed under strings as shorthand for a complex model hyperparameter configuration known to work well:\n",
       "            'GBM': ['GBMLarge']\n",
       "    Advanced functionality: Bring your own model / Custom model support\n",
       "        AutoGluon fully supports custom models. For a detailed tutorial on creating and using custom models with AutoGluon, refer to https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-custom-model.html\n",
       "    Advanced functionality: Custom stack levels\n",
       "        By default, AutoGluon re-uses the same models and model hyperparameters at each level during stack ensembling.\n",
       "        To customize this behaviour, create a hyperparameters dictionary separately for each stack level, and then add them as values to a new dictionary, with keys equal to the stack level.\n",
       "            Example: `hyperparameters = {1: {'RF': rf_params1}, 2: {'CAT': [cat_params1, cat_params2], 'NN_TORCH': {}}}`\n",
       "            This will result in a stack ensemble that has one custom random forest in level 1 followed by two CatBoost models with custom hyperparameters and a default neural network in level 2, for a total of 4 models.\n",
       "        If a level is not specified in `hyperparameters`, it will default to using the highest specified level to train models. This can also be explicitly controlled by adding a 'default' key.\n",
       "\n",
       "    Default:\n",
       "        hyperparameters = {\n",
       "            'NN_TORCH': {},\n",
       "            'GBM': [\n",
       "                {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n",
       "                {},\n",
       "                'GBMLarge',\n",
       "            ],\n",
       "            'CAT': {},\n",
       "            'XGB': {},\n",
       "            'FASTAI': {},\n",
       "            'RF': [\n",
       "                {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n",
       "                {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n",
       "                {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n",
       "            ],\n",
       "            'XT': [\n",
       "                {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n",
       "                {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n",
       "                {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n",
       "            ],\n",
       "            'KNN': [\n",
       "                {'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}},\n",
       "                {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}},\n",
       "            ],\n",
       "        }\n",
       "\n",
       "    Details regarding the hyperparameters you can specify for each model are provided in the following files:\n",
       "        NN: `autogluon.tabular.models.tabular_nn.hyperparameters.parameters`\n",
       "            Note: certain hyperparameter settings may cause these neural networks to train much slower.\n",
       "        GBM: `autogluon.tabular.models.lgb.hyperparameters.parameters`\n",
       "             See also the lightGBM docs: https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
       "        CAT: `autogluon.tabular.models.catboost.hyperparameters.parameters`\n",
       "             See also the CatBoost docs: https://catboost.ai/docs/concepts/parameter-tuning.html\n",
       "        XGB: `autogluon.tabular.models.xgboost.hyperparameters.parameters`\n",
       "             See also the XGBoost docs: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
       "        FASTAI: `autogluon.tabular.models.fastainn.hyperparameters.parameters`\n",
       "             See also the FastAI docs: https://docs.fast.ai/tabular.learner.html\n",
       "        RF: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
       "            Note: Hyperparameter tuning is disabled for this model.\n",
       "        XT: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
       "            Note: Hyperparameter tuning is disabled for this model.\n",
       "        KNN: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
       "            Note: Hyperparameter tuning is disabled for this model.\n",
       "        LR: `autogluon.tabular.models.lr.hyperparameters.parameters`\n",
       "            Note: Hyperparameter tuning is disabled for this model.\n",
       "            Note: 'penalty' parameter can be used for regression to specify regularization method: 'L1' and 'L2' values are supported.\n",
       "        Advanced functionality: Custom AutoGluon model arguments\n",
       "            These arguments are optional and can be specified in any model's hyperparameters.\n",
       "                Example: `hyperparameters = {'RF': {..., 'ag_args': {'name_suffix': 'CustomModelSuffix', 'disable_in_hpo': True}}`\n",
       "            ag_args: Dictionary of customization options related to meta properties of the model such as its name, the order it is trained, the problem types it is valid for, and the type of HPO it utilizes.\n",
       "                Valid keys:\n",
       "                    name: (str) The name of the model. This overrides AutoGluon's naming logic and all other name arguments if present.\n",
       "                    name_main: (str) The main name of the model. Example: 'RandomForest'.\n",
       "                    name_prefix: (str) Add a custom prefix to the model name. Unused by default.\n",
       "                    name_suffix: (str) Add a custom suffix to the model name. Unused by default.\n",
       "                    priority: (int) Determines the order in which the model is trained. Larger values result in the model being trained earlier. Default values range from 100 (KNN) to 0 (custom), dictated by model type. If you want this model to be trained first, set priority = 999.\n",
       "                    problem_types: (list) List of valid problem types for the model. `problem_types=['binary']` will result in the model only being trained if `problem_type` is 'binary'.\n",
       "                    disable_in_hpo: (bool) If True, the model will only be trained if `hyperparameter_tune_kwargs=None`.\n",
       "                    valid_stacker: (bool) If False, the model will not be trained as a level 2 or higher stacker model.\n",
       "                    valid_base: (bool) If False, the model will not be trained as a level 1 (base) model.\n",
       "                    hyperparameter_tune_kwargs: (dict) Refer to :meth:`TabularPredictor.fit` hyperparameter_tune_kwargs argument. If specified here, will override global HPO settings for this model.\n",
       "                Reference the default hyperparameters for example usage of these options.\n",
       "            ag_args_fit: Dictionary of model fit customization options related to how and with what constraints the model is trained. These parameters affect stacker fold models, but not stacker models themselves.\n",
       "                Clarification: `time_limit` is the internal time in seconds given to a particular model to train, which is dictated in part by the `time_limit` argument given during `predictor.fit()` but is not the same.\n",
       "                Valid keys:\n",
       "                    stopping_metric: (str or :class:`autogluon.core.metrics.Scorer`, default=None) The metric to use for early stopping of the model. If None, model will decide.\n",
       "                    max_memory_usage_ratio: (float, default=1.0) The ratio of memory usage relative to the default to allow before early stopping or killing the model. Values greater than 1.0 will be increasingly prone to out-of-memory errors.\n",
       "                    max_time_limit_ratio: (float, default=1.0) The ratio of the provided time_limit to use during model `fit()`. If `time_limit=10` and `max_time_limit_ratio=0.3`, time_limit would be changed to 3. Does not alter max_time_limit or min_time_limit values.\n",
       "                    max_time_limit: (float, default=None) Maximum amount of time to allow this model to train for (in sec). If the provided time_limit is greater than this value, it will be replaced by max_time_limit.\n",
       "                    min_time_limit: (float, default=0) Allow this model to train for at least this long (in sec), regardless of the time limit it would otherwise be granted.\n",
       "                        If `min_time_limit >= max_time_limit`, time_limit will be set to min_time_limit.\n",
       "                        If `min_time_limit=None`, time_limit will be set to None and the model will have no training time restriction.\n",
       "                    num_cpus : (int or str, default='auto')\n",
       "                        How many CPUs to use during model fit.\n",
       "                        If 'auto', model will decide.\n",
       "                    num_gpus : (int or str, default='auto')\n",
       "                        How many GPUs to use during model fit.\n",
       "                        If 'auto', model will decide. Some models can use GPUs but don't by default due to differences in model quality.\n",
       "                        Set to 0 to disable usage of GPUs.\n",
       "            ag_args_ensemble: Dictionary of hyperparameters shared by all models that control how they are ensembled, if bag mode is enabled.\n",
       "                Valid keys:\n",
       "                    use_orig_features: (bool) Whether a stack model will use the original features along with the stack features to train (akin to skip-connections). If the model has no stack features (no base models), this value is ignored and the stack model will use the original features.\n",
       "                    max_base_models: (int, default=25) Maximum number of base models whose predictions form the features input to this stacker model. If more than `max_base_models` base models are available, only the top `max_base_models` models with highest validation score are used.\n",
       "                    max_base_models_per_type: (int, default=5) Similar to `max_base_models`. If more than `max_base_models_per_type` of any particular model type are available, only the top `max_base_models_per_type` of that type are used. This occurs before the `max_base_models` filter.\n",
       "                    num_folds: (int, default=None) If specified, the number of folds to fit in the bagged model.\n",
       "                        If specified, overrides any other value used to determine the number of folds such as predictor.fit `num_bag_folds` argument.\n",
       "                    max_sets: (int, default=None) If specified, the maximum sets to fit in the bagged model.\n",
       "                        The lesser of `max_sets` and the predictor.fit `num_bag_sets` argument will be used for the given model.\n",
       "                        Useful if a particular model is expensive relative to others and you want to avoid repeated bagging of the expensive model while still repeated bagging the cheaper models.\n",
       "                    save_bag_folds: (bool, default=True)\n",
       "                        If True, bagged models will save their fold models (the models from each individual fold of bagging). This is required to use bagged models for prediction.\n",
       "                        If False, bagged models will not save their fold models. This means that bagged models will not be valid models during inference.\n",
       "                            This should only be set to False when planning to call `predictor.refit_full()` or when `refit_full` is set and `set_best_to_refit_full=True`.\n",
       "                            Particularly useful if disk usage is a concern. By not saving the fold models, bagged models will use only very small amounts of disk space during training.\n",
       "                            In many training runs, this will reduce peak disk usage by >10x.\n",
       "                    fold_fitting_strategy: (AbstractFoldFittingStrategy default=auto) Whether to fit folds in parallel or in sequential order.\n",
       "                        If parallel_local, folds will be trained in parallel with evenly distributed computing resources. This could bring 2-4x speedup compared to SequentialLocalFoldFittingStrategy, but could consume much more memory.\n",
       "                        If sequential_local, folds will be trained in sequential.\n",
       "                        If auto, strategy will be determined by OS and whether ray is installed or not. MacOS support for parallel_local is unstable, and may crash if enabled.\n",
       "                    num_folds_parallel: (int or str, default='auto') Number of folds to be trained in parallel if using ParallelLocalFoldFittingStrategy. Consider lowering this value if you encounter either out of memory issue or CUDA out of memory issue(when trained on gpu).\n",
       "                        if 'auto', will try to train all folds in parallel.\n",
       "\n",
       "feature_metadata : :class:`autogluon.tabular.FeatureMetadata` or str, default = 'infer'\n",
       "    The feature metadata used in various inner logic in feature preprocessing.\n",
       "    If 'infer', will automatically construct a FeatureMetadata object based on the properties of `train_data`.\n",
       "    In this case, `train_data` is input into :meth:`autogluon.tabular.FeatureMetadata.from_df` to infer `feature_metadata`.\n",
       "    If 'infer' incorrectly assumes the dtypes of features, consider explicitly specifying `feature_metadata`.\n",
       "infer_limit : float, default = None\n",
       "    The inference time limit in seconds per row to adhere to during fit.\n",
       "    If infer_limit=0.05 and infer_limit_batch_size=1000, AutoGluon will avoid training models that take longer than 50 ms/row to predict when given a batch of 1000 rows to predict (must predict 1000 rows in no more than 50 seconds).\n",
       "    If bagging is enabled, the inference time limit will be respected based on estimated inference speed of `_FULL` models after refit_full is called, NOT on the inference speed of the bagged ensembles.\n",
       "    The inference times calculated for models are assuming `predictor.persist('all')` is called after fit.\n",
       "    If None, no limit is enforced.\n",
       "    If it is impossible to satisfy the constraint, an exception will be raised.\n",
       "infer_limit_batch_size : int, default = None\n",
       "    The batch size to use when predicting in bulk to estimate per-row inference time.\n",
       "    Must be an integer greater than 0.\n",
       "    If None and `infer_limit` is specified, will default to 10000.\n",
       "    It is recommended to set to 10000 unless you must satisfy an online-inference scenario.\n",
       "    Small values, especially `infer_limit_batch_size=1`, will result in much larger per-row inference times and should be avoided if possible.\n",
       "    Refer to `infer_limit` for more details on how this is used.\n",
       "    If specified when `infer_limit=None`, the inference time will be logged during training but will not be limited.\n",
       "fit_weighted_ensemble : bool, default = True\n",
       "    If True, a WeightedEnsembleModel will be fit in each stack layer.\n",
       "    A weighted ensemble will often be stronger than an individual model while being very fast to train.\n",
       "    It is recommended to keep this value set to True to maximize predictive quality.\n",
       "fit_full_last_level_weighted_ensemble : bool, default = True\n",
       "    If True, the WeightedEnsembleModel of the last stacking level will be fit with all (successful) models from all previous layers as base models.\n",
       "    If stacking is disabled, settings this to True or False makes no difference because the WeightedEnsembleModel L2 always uses all models from L1.\n",
       "    It is recommended to keep this value set to True to maximize predictive quality.\n",
       "full_weighted_ensemble_additionally : bool, default = False\n",
       "    If True, AutoGluon will fit two WeightedEnsembleModels after training all stacking levels. Setting this to True, simulates calling\n",
       "    `fit_weighted_ensemble()` after calling `fit()`. Has no affect if `fit_full_last_level_weighted_ensemble` is False and does not fit an additional\n",
       "    WeightedEnsembleModel if stacking is disabled.\n",
       "dynamic_stacking: bool | str, default = False\n",
       "    If True and `num_stack_levels` > 0, AutoGluon will dynamically determine whether to use stacking or not by first validating AutoGluon's stacking\n",
       "    behavior. This is done to avoid so-called stacked overfitting that can make traditional multi-layer stacking, as used in AutoGluon, fail drastically\n",
       "    and produce unreliable validation scores.\n",
       "    It is recommended to keep this value set to True or \"auto\" when using stacking,\n",
       "    as long as it is unknown whether the data is affected by stacked overfitting.\n",
       "    If it is known that the data is unaffected by stacked overfitting, then setting this value to False is expected to maximize predictive quality.\n",
       "    If enabled, by default, AutoGluon performs dynamic stacking by spending 25% of the provided time limit for detection and all remaining\n",
       "    time for fitting AutoGluon. This can be adjusted by specifying `ds_args` with different parameters to `fit()`.\n",
       "    If \"auto\", will be set to `not use_bag_holdout`.\n",
       "    See the documentation of `ds_args` for more information.\n",
       "calibrate_decision_threshold : bool, default = False\n",
       "    [Experimental] This may be removed / changed without warning in a future release.\n",
       "    If True, will automatically calibrate the decision threshold at the end of fit for calls to `.predict` based on the evaluation metric.\n",
       "    By default, the decision threshold is `0.5`, however for some metrics such as `f1` and `balanced_accuracy`,\n",
       "    scores can be significantly improved by choosing a threshold other than `0.5`.\n",
       "    Only valid for `problem_type='binary'`. Ignored for all other problem types.\n",
       "num_cpus: int, default = \"auto\"\n",
       "    The total amount of cpus you want AutoGluon predictor to use.\n",
       "    Auto means AutoGluon will make the decision based on the total number of cpus available and the model requirement for best performance.\n",
       "    Users generally don't need to set this value\n",
       "num_gpus: int, default = \"auto\"\n",
       "    The total amount of gpus you want AutoGluon predictor to use.\n",
       "    Auto means AutoGluon will make the decision based on the total number of gpus available and the model requirement for best performance.\n",
       "    Users generally don't need to set this value\n",
       "**kwargs :\n",
       "    auto_stack : bool, default = False\n",
       "        Whether AutoGluon should automatically utilize bagging and multi-layer stack ensembling to boost predictive accuracy.\n",
       "        Set this = True if you are willing to tolerate longer training times in order to maximize predictive accuracy!\n",
       "        Automatically sets `num_bag_folds` and `num_stack_levels` arguments based on dataset properties.\n",
       "        Note: Setting `num_bag_folds` and `num_stack_levels` arguments will override `auto_stack`.\n",
       "        Note: This can increase training time (and inference time) by up to 20x, but can greatly improve predictive performance.\n",
       "    num_bag_folds : int, default = None\n",
       "        Number of folds used for bagging of models. When `num_bag_folds = k`, training time is roughly increased by a factor of `k` (set = 0 to disable bagging).\n",
       "        Disabled by default (0), but we recommend values between 5-10 to maximize predictive performance.\n",
       "        Increasing num_bag_folds will result in models with lower bias but that are more prone to overfitting.\n",
       "        `num_bag_folds = 1` is an invalid value, and will raise a ValueError.\n",
       "        Values > 10 may produce diminishing returns, and can even harm overall results due to overfitting.\n",
       "        To further improve predictions, avoid increasing `num_bag_folds` much beyond 10 and instead increase `num_bag_sets`.\n",
       "    num_bag_sets : int, default = None\n",
       "        Number of repeats of kfold bagging to perform (values must be >= 1). Total number of models trained during bagging = `num_bag_folds * num_bag_sets`.\n",
       "        Defaults to 1 if `time_limit` is not specified, otherwise 20 (always disabled if `num_bag_folds` is not specified).\n",
       "        Values greater than 1 will result in superior predictive performance, especially on smaller problems and with stacking enabled (reduces overall variance).\n",
       "    num_stack_levels : int, default = None\n",
       "        Number of stacking levels to use in stack ensemble. Roughly increases model training time by factor of `num_stack_levels+1` (set = 0 to disable stack ensembling).\n",
       "        Disabled by default (0), but we recommend values between 1-3 to maximize predictive performance.\n",
       "        To prevent overfitting, `num_bag_folds >= 2` must also be set or else a ValueError will be raised.\n",
       "    holdout_frac : float, default = None\n",
       "        Fraction of train_data to holdout as tuning data for optimizing hyperparameters (ignored unless `tuning_data = None`, ignored if `num_bag_folds != 0` unless `use_bag_holdout == True`).\n",
       "        Default value (if None) is selected based on the number of rows in the training data. Default values range from 0.2 at 2,500 rows to 0.01 at 250,000 rows.\n",
       "        Default value is doubled if `hyperparameter_tune_kwargs` is set, up to a maximum of 0.2.\n",
       "        Disabled if `num_bag_folds >= 2` unless `use_bag_holdout == True`.\n",
       "    use_bag_holdout : bool | str, default = False\n",
       "        If True, a `holdout_frac` portion of the data is held-out from model bagging.\n",
       "        This held-out data is only used to score models and determine weighted ensemble weights.\n",
       "        Enable this if there is a large gap between score_val and score_test in stack models.\n",
       "        Note: If `tuning_data` was specified, `tuning_data` is used as the holdout data.\n",
       "        Disabled if not bagging.\n",
       "        If \"auto\", will be set to True if the training data has >= 1000000 rows, else it will be set to False.\n",
       "    hyperparameter_tune_kwargs : str or dict, default = None\n",
       "        Hyperparameter tuning strategy and kwargs (for example, how many HPO trials to run).\n",
       "        If None, then hyperparameter tuning will not be performed.\n",
       "        You can either choose to provide a preset\n",
       "            Valid preset values:\n",
       "                'auto': Performs HPO via bayesian optimization search on NN_TORCH and FASTAI models, and random search on other models using local scheduler.\n",
       "                'random': Performs HPO via random search using local scheduler.\n",
       "        Or provide a dict to specify searchers and schedulers\n",
       "            Valid keys:\n",
       "                'num_trials': How many HPO trials to run\n",
       "                'scheduler': Which scheduler to use\n",
       "                    Valid values:\n",
       "                        'local': Local shceduler that schedule trials FIFO\n",
       "                'searcher': Which searching algorithm to use\n",
       "                    'local_random': Uses the 'random' searcher\n",
       "                    'random': Perform random search\n",
       "                    'auto': Perform bayesian optimization search on NN_TORCH and FASTAI models. Perform random search on other models.\n",
       "            The 'scheduler' and 'searcher' key are required when providing a dict.\n",
       "            An example of a valid dict:\n",
       "                hyperparameter_tune_kwargs = {\n",
       "                    'num_trials': 5,\n",
       "                    'scheduler' : 'local',\n",
       "                    'searcher': 'auto',\n",
       "                }\n",
       "    feature_prune_kwargs: dict, default = None\n",
       "        Performs layer-wise feature pruning via recursive feature elimination with permutation feature importance.\n",
       "        This fits all models in a stack layer once, discovers a pruned set of features, fits all models in the stack layer\n",
       "        again with the pruned set of features, and updates input feature lists for models whose validation score improved.\n",
       "        If None, do not perform feature pruning. If empty dictionary, perform feature pruning with default configurations.\n",
       "        For valid dictionary keys, refer to :class:`autogluon.core.utils.feature_selection.FeatureSelector` and\n",
       "        `autogluon.core.trainer.abstract_trainer.AbstractTrainer._proxy_model_feature_prune` documentation.\n",
       "        To force all models to work with the pruned set of features, set force_prune=True in the dictionary.\n",
       "    ag_args : dict, default = None\n",
       "        Keyword arguments to pass to all models (i.e. common hyperparameters shared by all AutoGluon models).\n",
       "        See the `ag_args` argument from \"Advanced functionality: Custom AutoGluon model arguments\" in the `hyperparameters` argument documentation for valid values.\n",
       "        Identical to specifying `ag_args` parameter for all models in `hyperparameters`.\n",
       "        If a key in `ag_args` is already specified for a model in `hyperparameters`, it will not be altered through this argument.\n",
       "    ag_args_fit : dict, default = None\n",
       "        Keyword arguments to pass to all models.\n",
       "        See the `ag_args_fit` argument from \"Advanced functionality: Custom AutoGluon model arguments\" in the `hyperparameters` argument documentation for valid values.\n",
       "        Identical to specifying `ag_args_fit` parameter for all models in `hyperparameters`.\n",
       "        If a key in `ag_args_fit` is already specified for a model in `hyperparameters`, it will not be altered through this argument.\n",
       "    ag_args_ensemble : dict, default = None\n",
       "        Keyword arguments to pass to all models.\n",
       "        See the `ag_args_ensemble` argument from \"Advanced functionality: Custom AutoGluon model arguments\" in the `hyperparameters` argument documentation for valid values.\n",
       "        Identical to specifying `ag_args_ensemble` parameter for all models in `hyperparameters`.\n",
       "        If a key in `ag_args_ensemble` is already specified for a model in `hyperparameters`, it will not be altered through this argument.\n",
       "    ds_args : dict, see below for default\n",
       "        Keyword arguments for dynamic stacking, only used if `dynamic_stacking=True`. These keyword arguments control the behavior of dynamic stacking\n",
       "        and determine how AutoGluon tries to detect stacked overfitting. To detect stacked overfitting, AutoGluon will fit itself (so called sub-fits)\n",
       "        on a subset (for holdout) or multiple subsets (for repeated cross-validation) and use the predictions of AutoGluon on the validation data to\n",
       "        detect stacked overfitting. The sub-fits stop and stacking will be disabled if any sub-fit shows stacked overfitting.\n",
       "        Allowed keys and values are:\n",
       "            `detection_time_frac` : float in (0,1), default = 1/4\n",
       "                Determines how much of the original training time is used for detecting stacked overfitting.\n",
       "                When using (repeated) cross-validation, each sub-fit will be fit for `1/n_splits * detection_time_frac * time_limit`.\n",
       "                If no time limit is given to AutoGluon, this parameter is ignored and AutoGluon is fit without a time limit in the sub-fit.\n",
       "            `validation_procedure`: str, default = 'holdout'\n",
       "                Determines the validation procedure used to detect stacked overfitting. Can be either `cv` or `holdout`.\n",
       "                    If `validation_procedure='holdout'` and `holdout_data` is not specified (default), then `holdout_frac` determines the holdout data.\n",
       "                    If `validation_procedure='holdout'` and `holdout_data` is specified, then the provided `holdout_data` is used for validation.\n",
       "                    If `validation_procedure='cv'`, `n_folds` and `n_repeats` determine the kind cross-validation procedure.\n",
       "            `holdout_frac` : float in (0,1), default = 1/9\n",
       "                Determines how much of the original training data is used for the holdout data during holdout validation.\n",
       "                Ignored if `holdout_data` is not None.\n",
       "            `n_folds` : int in [2, +inf), default = 2\n",
       "                Number of folds to use for cross-validation.\n",
       "            `n_repeats` : int [1, +inf), default = 1\n",
       "                Number of repeats to use for repeated cross-validation. If set to 1, performs 1-repeated cross-validation which is equivalent to\n",
       "                cross-validation without repeats.\n",
       "            `memory_safe_fits` : bool, default = True\n",
       "                If True, AutoGluon runs each sub-fit in a ray-based subprocess to avoid memory leakage that exist due to Python's lackluster\n",
       "                garbage collector.\n",
       "            `clean_up_fits` : bool, default = True\n",
       "                If True, AutoGluon will remove all saved information from sub-fits from disk.\n",
       "                If False, the sub-fits are kept on disk and `self._sub_fits` will store paths to the sub-fits, which can be loaded just like any other\n",
       "                predictor from disk using `TabularPredictor.load()`.\n",
       "            `holdout_data`: str or :class:`TabularDataset` or :class:`pd.DataFrame`, default = None\n",
       "                Another dataset containing validation data reserved for detecting stacked overfitting. This dataset should be in the same format as\n",
       "                `train_data`. If str is passed, `holdout_data` will be loaded using the str value as the file path.\n",
       "                If `holdout_data` is not None, the sub-fit is fit on all of `train_data` and the full fit is fit on all of `train_data` and\n",
       "                `holdout_data` combined.\n",
       "    included_model_types : list, default = None\n",
       "        To only include listed model types for training during `fit()`.\n",
       "        Models that are listed in `included_model_types` but not in `hyperparameters` will be ignored.\n",
       "        Reference `hyperparameters` documentation for what models correspond to each value.\n",
       "        Useful when only a subset of model needs to be trained and the `hyperparameters` dictionary is difficult or time-consuming.\n",
       "            Example: To include both 'GBM' and 'FASTAI' models, specify `included_model_types=['GBM', 'FASTAI']`.\n",
       "    excluded_model_types : list, default = None\n",
       "        Banned subset of model types to avoid training during `fit()`, even if present in `hyperparameters`.\n",
       "        Reference `hyperparameters` documentation for what models correspond to each value.\n",
       "        Useful when a particular model type such as 'KNN' or 'custom' is not desired but altering the `hyperparameters` dictionary is difficult or time-consuming.\n",
       "            Example: To exclude both 'KNN' and 'custom' models, specify `excluded_model_types=['KNN', 'custom']`.\n",
       "    refit_full : bool or str, default = False\n",
       "        Whether to retrain all models on all of the data (training + validation) after the normal training procedure.\n",
       "        This is equivalent to calling `predictor.refit_full(model=refit_full)` after fit.\n",
       "        If `refit_full=True`, it will be treated as `refit_full='all'`.\n",
       "        If `refit_full=False`, refitting will not occur.\n",
       "        Valid str values:\n",
       "            `all`: refits all models.\n",
       "            `best`: refits only the best model (and its ancestors if it is a stacker model).\n",
       "            `{model_name}`: refits only the specified model (and its ancestors if it is a stacker model).\n",
       "        For bagged models:\n",
       "            Reduces a model's inference time by collapsing bagged ensembles into a single model fit on all of the training data.\n",
       "            This process will typically result in a slight accuracy reduction and a large inference speedup.\n",
       "            The inference speedup will generally be between 10-200x faster than the original bagged ensemble model.\n",
       "                The inference speedup factor is equivalent to (k * n), where k is the number of folds (`num_bag_folds`) and n is the number of finished repeats (`num_bag_sets`) in the bagged ensemble.\n",
       "            The runtime is generally 10% or less of the original fit runtime.\n",
       "                The runtime can be roughly estimated as 1 / (k * n) of the original fit runtime, with k and n defined above.\n",
       "        For non-bagged models:\n",
       "            Optimizes a model's accuracy by retraining on 100% of the data without using a validation set.\n",
       "            Will typically result in a slight accuracy increase and no change to inference time.\n",
       "            The runtime will be approximately equal to the original fit runtime.\n",
       "        This process does not alter the original models, but instead adds additional models.\n",
       "        If stacker models are refit by this process, they will use the refit_full versions of the ancestor models during inference.\n",
       "        Models produced by this process will not have validation scores, as they use all of the data for training.\n",
       "            Therefore, it is up to the user to determine if the models are of sufficient quality by including test data in `predictor.leaderboard(test_data)`.\n",
       "            If the user does not have additional test data, they should reference the original model's score for an estimate of the performance of the refit_full model.\n",
       "                Warning: Be aware that utilizing refit_full models without separately verifying on test data means that the model is untested, and has no guarantee of being consistent with the original model.\n",
       "        The time taken by this process is not enforced by `time_limit`.\n",
       "    save_bag_folds : bool, default = True\n",
       "        If True, will save the bagged fold models to disk.\n",
       "        If False, will not save the bagged fold models, only keeping their metadata and out-of-fold predictions.\n",
       "            Note: The bagged models will not be available for prediction, only use this if you intend to call `refit_full`.\n",
       "            The purpose of setting it to False is that it greatly decreases the peak disk usage of the predictor during the fit call when bagging.\n",
       "            Note that this makes refit_full slightly more likely to crash in scenarios where the dataset is large relative to available system memory.\n",
       "            This is because by default, refit_full will fall back to cloning the first fold of the bagged model in case it lacks memory to refit.\n",
       "            However, if `save_bag_folds=False`, this fallback isn't possible, as there is not fold model to clone because it wasn't saved.\n",
       "            In this scenario, refit will raise an exception for `save_bag_folds=False`, but will succeed if `save_bag_folds=True`.\n",
       "        Final disk usage of predictor will be identical regardless of the setting after `predictor.delete_models(models_to_keep=\"best\", dry_run=False)` is called post-fit.\n",
       "    set_best_to_refit_full : bool, default = False\n",
       "        If True, will change the default model that Predictor uses for prediction when model is not specified to the refit_full version of the model that exhibited the highest validation score.\n",
       "        Only valid if `refit_full` is set.\n",
       "    keep_only_best : bool, default = False\n",
       "        If True, only the best model and its ancestor models are saved in the outputted `predictor`. All other models are deleted.\n",
       "            If you only care about deploying the most accurate predictor with the smallest file-size and no longer need any of the other trained models or functionality beyond prediction on new data, then set: `keep_only_best=True`, `save_space=True`.\n",
       "            This is equivalent to calling `predictor.delete_models(models_to_keep='best', dry_run=False)` directly after `fit()`.\n",
       "        If used with `refit_full` and `set_best_to_refit_full`, the best model will be the refit_full model, and the original bagged best model will be deleted.\n",
       "            `refit_full` will be automatically set to 'best' in this case to avoid training models which will be later deleted.\n",
       "    save_space : bool, default = False\n",
       "        If True, reduces the memory and disk size of predictor by deleting auxiliary model files that aren't needed for prediction on new data.\n",
       "            This is equivalent to calling `predictor.save_space()` directly after `fit()`.\n",
       "        This has NO impact on inference accuracy.\n",
       "        It is recommended if the only goal is to use the trained model for prediction.\n",
       "        Certain advanced functionality may no longer be available if `save_space=True`. Refer to `predictor.save_space()` documentation for more details.\n",
       "    feature_generator : :class:`autogluon.features.generators.AbstractFeatureGenerator`, default = :class:`autogluon.features.generators.AutoMLPipelineFeatureGenerator`\n",
       "        The feature generator used by AutoGluon to process the input data to the form sent to the models. This often includes automated feature generation and data cleaning.\n",
       "        It is generally recommended to keep the default feature generator unless handling an advanced use-case.\n",
       "        To control aspects of the default feature generation process, you can pass in an :class:`AutoMLPipelineFeatureGenerator` object constructed using some of these kwargs:\n",
       "            enable_numeric_features : bool, default True\n",
       "                Whether to keep features of 'int' and 'float' raw types.\n",
       "                These features are passed without alteration to the models.\n",
       "                Appends IdentityFeatureGenerator(infer_features_in_args=dict(valid_raw_types=['int', 'float']))) to the generator group.\n",
       "            enable_categorical_features : bool, default True\n",
       "                Whether to keep features of 'object' and 'category' raw types.\n",
       "                These features are processed into memory optimized 'category' features.\n",
       "                Appends CategoryFeatureGenerator() to the generator group.\n",
       "            enable_datetime_features : bool, default True\n",
       "                Whether to keep features of 'datetime' raw type and 'object' features identified as 'datetime_as_object' features.\n",
       "                These features will be converted to 'int' features representing milliseconds since epoch.\n",
       "                Appends DatetimeFeatureGenerator() to the generator group.\n",
       "            enable_text_special_features : bool, default True\n",
       "                Whether to use 'object' features identified as 'text' features to generate 'text_special' features such as word count, capital letter ratio, and symbol counts.\n",
       "                Appends TextSpecialFeatureGenerator() to the generator group.\n",
       "            enable_text_ngram_features : bool, default True\n",
       "                Whether to use 'object' features identified as 'text' features to generate 'text_ngram' features.\n",
       "                Appends TextNgramFeatureGenerator(vectorizer=vectorizer) to the generator group.\n",
       "            enable_raw_text_features : bool, default False\n",
       "                Whether to keep the raw text features.\n",
       "                Appends IdentityFeatureGenerator(infer_features_in_args=dict(required_special_types=['text'])) to the generator group.\n",
       "            vectorizer : CountVectorizer, default CountVectorizer(min_df=30, ngram_range=(1, 3), max_features=10000, dtype=np.uint8)\n",
       "                sklearn CountVectorizer object to use in TextNgramFeatureGenerator.\n",
       "                Only used if `enable_text_ngram_features=True`.\n",
       "    unlabeled_data : pd.DataFrame, default = None\n",
       "        [Experimental Parameter]\n",
       "        Collection of data without labels that we can use to pretrain on. This is the same schema as train_data, except\n",
       "        without the labels. Currently, unlabeled_data is only used for pretraining a TabTransformer model.\n",
       "        If you do not specify 'TRANSF' with unlabeled_data, then no pretraining will occur and unlabeled_data will be ignored!\n",
       "        After the pretraining step, we will finetune using the TabTransformer model as well. If TabTransformer is ensembled\n",
       "        with other models, like in typical AutoGluon fashion, then the output of this \"pretrain/finetune\" will be ensembled\n",
       "        with other models, which will not used the unlabeled_data. The \"pretrain/finetune flow\" is also known as semi-supervised learning.\n",
       "        The typical use case for unlabeled_data is to add signal to your model where you may not have sufficient training\n",
       "        data. e.g. 500 hand-labeled samples (perhaps a hard human task), whole data set (unlabeled) is thousands/millions.\n",
       "        However, this isn't the only use case. Given enough unlabeled data(millions of rows), you may see improvements\n",
       "        to any amount of labeled data.\n",
       "    verbosity : int\n",
       "        If specified, overrides the existing `predictor.verbosity` value.\n",
       "    calibrate: bool or str, default = 'auto'\n",
       "        Note: It is recommended to use ['auto', False] as the values and avoid True.\n",
       "        If 'auto' will automatically set to True if the problem_type and eval_metric are suitable for calibration.\n",
       "        If True and the problem_type is classification, temperature scaling will be used to calibrate the Predictor's estimated class probabilities\n",
       "        (which may improve metrics like log_loss) and will train a scalar parameter on the validation set.\n",
       "        If True and the problem_type is quantile regression, conformalization will be used to calibrate the Predictor's estimated quantiles\n",
       "        (which may improve the prediction interval coverage, and bagging could further improve it) and will compute a set of scalar parameters on the validation set.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`TabularPredictor` object. Returns self.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from autogluon.tabular import TabularDataset, TabularPredictor\n",
       ">>> train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
       ">>> label = 'class'\n",
       ">>> predictor = TabularPredictor(label=label).fit(train_data)\n",
       ">>> test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
       ">>> leaderboard = predictor.leaderboard(test_data)\n",
       ">>> y_test = test_data[label]\n",
       ">>> test_data = test_data.drop(columns=[label])\n",
       ">>> y_pred = predictor.predict(test_data)\n",
       ">>> perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred)\n",
       "\n",
       "To maximize predictive performance, use the following:\n",
       "\n",
       ">>> eval_metric = 'roc_auc'  # set this to the metric you ultimately care about\n",
       ">>> time_limit = 3600  # set as long as you are willing to wait (in sec)\n",
       ">>> predictor = TabularPredictor(label=label, eval_metric=eval_metric).fit(train_data, presets=['best_quality'], time_limit=time_limit)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/py38/lib/python3.8/site-packages/autogluon/tabular/predictor/predictor.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tp = TabularPredictor(verbosity=3, label='Target')\n",
    "tp.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e2d18c35-f70d-4dbc-b350-b6760f0bb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'GBM': {},\n",
    "    'NN_TORCH': {},\n",
    "    'FASTAI': {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "55f1eb9a-c8c1-4f5c-87e3-231dd2725294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240628_085151\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240628_085151\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Sun Jan 19 18:21:42 CST 2020\n",
      "CPU Count:          64\n",
      "Memory Avail:       92.69 GB / 251.62 GB (36.8%)\n",
      "Disk Space Avail:   105.79 GB / 879.22 GB (12.0%)\n",
      "===================================================\n",
      "Train Data Rows:    76518\n",
      "Train Data Columns: 36\n",
      "Label Column:       Target\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    94934.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11.82 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 18 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Daytime_evening_attendance', ...]\n",
      "\t\t('float', [])    :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])      : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['Marital_status', 'Application_mode', 'Application_order', 'Course', 'Previous_qualification', ...]\n",
      "\t\t('float', [])     :  7 | ['Previous_qualification_grade', 'Admission_grade', 'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade', 'Unemployment_rate', ...]\n",
      "\t\t('int', [])       : 11 | ['Age_at_enrollment', 'Curricular_units_1st_sem_credited', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_1st_sem_approved', ...]\n",
      "\t\t('int', ['bool']) :  8 | ['Daytime_evening_attendance', 'Displaced', 'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t36 features in original data used to generate 36 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 11.83 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.35s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': {},\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "}\n",
      "Fitting 3 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=12, gpus=0, memory=0.15%)\n",
      "\t0.8269\t = Validation score   (accuracy)\n",
      "\t122.15s\t = Training   runtime\n",
      "\t1.62s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=12, gpus=0, memory=0.10%)\n",
      "\t0.8327\t = Validation score   (accuracy)\n",
      "\t7.87s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=12, gpus=0, memory=0.08%)\n",
      "\t0.8259\t = Validation score   (accuracy)\n",
      "\t125.68s\t = Training   runtime\n",
      "\t0.63s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.8327\t = Validation score   (accuracy)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 261.23s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240628_085151\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label='Target',\n",
    "    problem_type='multiclass',\n",
    "    eval_metric='accuracy',\n",
    "    verbosity=2,\n",
    ").fit(\n",
    "    train_df, \n",
    "    hyperparameters=hyperparameters,\n",
    "    num_bag_sets=1,\n",
    "    num_bag_folds=5,\n",
    "    num_stack_levels=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4549c033-59b1-4554-99d9-0c825017152a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.832719</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.725317</td>\n",
       "      <td>7.870559</td>\n",
       "      <td>0.725317</td>\n",
       "      <td>7.870559</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.832719</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.733454</td>\n",
       "      <td>8.595600</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>0.725040</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1</td>\n",
       "      <td>0.826930</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.618734</td>\n",
       "      <td>122.150313</td>\n",
       "      <td>1.618734</td>\n",
       "      <td>122.150313</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeuralNetTorch_BAG_L1</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.625731</td>\n",
       "      <td>125.676896</td>\n",
       "      <td>0.625731</td>\n",
       "      <td>125.676896</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  score_val eval_metric  pred_time_val    fit_time  \\\n",
       "0         LightGBM_BAG_L1   0.832719    accuracy       0.725317    7.870559   \n",
       "1     WeightedEnsemble_L2   0.832719    accuracy       0.733454    8.595600   \n",
       "2  NeuralNetFastAI_BAG_L1   0.826930    accuracy       1.618734  122.150313   \n",
       "3   NeuralNetTorch_BAG_L1   0.825871    accuracy       0.625731  125.676896   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.725317           7.870559            1       True   \n",
       "1                0.008137           0.725040            2       True   \n",
       "2                1.618734         122.150313            1       True   \n",
       "3                0.625731         125.676896            1       True   \n",
       "\n",
       "   fit_order  \n",
       "0          2  \n",
       "1          4  \n",
       "2          1  \n",
       "3          3  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801b42c-72d5-4394-935f-4afa4bda2f06",
   "metadata": {},
   "source": [
    "## OpenFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2a37dd-0696-4023-82f8-518d075cccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jiazhuang/opt/miniconda3/envs/py3.10/lib/python3.10/site-packages/openfe/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import openfe\n",
    "print(openfe.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7817d0-3fd1-4e82-a4f3-e58b48f80c1f",
   "metadata": {},
   "source": [
    "修改 /<miniconda安装目录>/envs/py38/lib/python3.8/site-packages/openfe/FeatureGenerator.py文件\n",
    "\n",
    "第6行修改为：\n",
    "\n",
    "num_operators = [\"abs\", \"log\", \"sqrt\", \"square\", \"sigmoid\", \"round\", \"residual\", \"<p0.2\", \"<p0.4\", \"<p0.6\", \"<p0.8\",]\n",
    "在第63行后增加：\n",
    "\n",
    "            elif self.name == '<p0.2':\n",
    "                new_data = (d < d.quantile(0.2).max()).astype(int)\n",
    "            elif self.name == '<p0.4':\n",
    "                new_data = (d < d.quantile(0.4).max()).astype(int)\n",
    "            elif self.name == '<p0.6':\n",
    "                new_data = (d < d.quantile(0.6).max()).astype(int)\n",
    "            elif self.name == '<p0.8':\n",
    "                new_data = (d < d.quantile(0.8).max()).astype(int)\n",
    "重启 notebook kernel 生效！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8600742c-6645-45d1-8f4c-a0fcdf1c9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfe import OpenFE, transform, get_candidate_features, tree_to_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57d81a7a-c850-4854-a904-e9924680b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_features = get_candidate_features(numerical_features=ori_num_feats, categorical_features=ori_cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85b48791-40f9-41ed-a432-69396be276dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3708"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidate_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf01aa7f-3180-4ce6-9405-1a0d688da712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict Search Space of Candidate Features\n",
    "candidate_features = [\n",
    "    f\n",
    "    for f in candidate_features\n",
    "    if f.name\n",
    "    in {\n",
    "        # \"abs\" -> dataset specific, not useful in most cases\n",
    "        # \"log\" -> can be done by scalers, no need for GBDTs\n",
    "        # \"sqrt\", -> see above (s.a.)\n",
    "        # \"square\" , -> s.a.\n",
    "        # \"sigmoid\" , -> s.a.\n",
    "        \"freq\",\n",
    "        \"round\",\n",
    "        \"residual\",\n",
    "        # \"max\", -> IMO, trivial to model for first-order features \n",
    "        # \"min\", -> s.a.\n",
    "        \"+\",\n",
    "        \"-\",\n",
    "        \"/\",\n",
    "        \"*\",\n",
    "        \"GroupByThenMin\",  # -> &nbsp;The essential benefit of GroupBy is captured with any of these, so I filtered this to reduce the search space.\n",
    "        \"GroupByThenMax\",  # -> s.a.\n",
    "        # \"GroupByThenMean\", -> s.a.\n",
    "        \"GroupByThenMedian\",\n",
    "        \"GroupByThenStd\",\n",
    "        \"GroupByThenRank\",\n",
    "        \"GroupByThenFreq\",\n",
    "        \"GroupByThenNUnique\",\n",
    "        \"Combine\",\n",
    "        # New Generators \n",
    "        #   - Hacked into OpenFE by adding `new_data = int(d < d.quantile(X).max())` to the generator options. \n",
    "        \"<p0.2\",  # X = 0.2\n",
    "        \"<p0.4\",   \n",
    "        \"<p0.6\",\n",
    "        \"<p0.8\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c1bb98d-7a7d-479f-8de2-b0d51b0b721c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2835"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidate_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4efae462-67a4-4dee-b057-3f20d1773c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train_df[ori_all_feats], train_df[['Target']]\n",
    "test_x = test_df[ori_all_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e57c5-19da-4221-b088-87bf3050ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ofe = OpenFE()\n",
    "\n",
    "%%capture --no-stderr\n",
    "\n",
    "features = ofe.fit(\n",
    "    data=train_x,\n",
    "    label=train_y,\n",
    "    task='classification',\n",
    "    candidate_features_list=candidate_features,\n",
    "    metric='multi_logloss',\n",
    "    n_data_blocks=2,\n",
    "    min_candidate_features=30000,\n",
    "    n_jobs=32,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd292304-22ea-44cf-a383-de7d6558cb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1841"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2173dfe-eec6-4df2-bf45-56a2c46f16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x = transform(train_x, test_x, features, n_jobs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57a225e2-c66f-4274-b536-c8b7a06c5569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marital_status</th>\n",
       "      <th>Application_mode</th>\n",
       "      <th>Application_order</th>\n",
       "      <th>Course</th>\n",
       "      <th>Daytime_evening_attendance</th>\n",
       "      <th>Previous_qualification</th>\n",
       "      <th>Previous_qualification_grade</th>\n",
       "      <th>Nacionality</th>\n",
       "      <th>Mother_s_qualification</th>\n",
       "      <th>Father_s_qualification</th>\n",
       "      <th>...</th>\n",
       "      <th>autoFE_f_1831</th>\n",
       "      <th>autoFE_f_1832</th>\n",
       "      <th>autoFE_f_1833</th>\n",
       "      <th>autoFE_f_1834</th>\n",
       "      <th>autoFE_f_1835</th>\n",
       "      <th>autoFE_f_1836</th>\n",
       "      <th>autoFE_f_1837</th>\n",
       "      <th>autoFE_f_1838</th>\n",
       "      <th>autoFE_f_1839</th>\n",
       "      <th>autoFE_f_1840</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476095</td>\n",
       "      <td>1.061701</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.676337</td>\n",
       "      <td>112006.0</td>\n",
       "      <td>58584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476095</td>\n",
       "      <td>0.872394</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.676337</td>\n",
       "      <td>112006.0</td>\n",
       "      <td>27730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.497029</td>\n",
       "      <td>1.250124</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.087940</td>\n",
       "      <td>112006.0</td>\n",
       "      <td>27730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498465</td>\n",
       "      <td>0.872394</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.724530</td>\n",
       "      <td>112006.0</td>\n",
       "      <td>58584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.497029</td>\n",
       "      <td>0.872394</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.087940</td>\n",
       "      <td>112006.0</td>\n",
       "      <td>58584.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1877 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Marital_status  Application_mode  Application_order  Course  \\\n",
       "id                                                                \n",
       "0                0                 0                  1      14   \n",
       "1                0                12                  1      14   \n",
       "2                0                12                  2      15   \n",
       "3                0                 0                  3      17   \n",
       "4                0                 0                  2      17   \n",
       "\n",
       "    Daytime_evening_attendance  Previous_qualification  \\\n",
       "id                                                       \n",
       "0                            1                       0   \n",
       "1                            1                       0   \n",
       "2                            1                       0   \n",
       "3                            1                       0   \n",
       "4                            1                       0   \n",
       "\n",
       "    Previous_qualification_grade  Nacionality  Mother_s_qualification  \\\n",
       "id                                                                      \n",
       "0                          126.0            0                       0   \n",
       "1                          125.0            0                      16   \n",
       "2                          137.0            0                       2   \n",
       "3                          131.0            0                      16   \n",
       "4                          132.0            0                      16   \n",
       "\n",
       "    Father_s_qualification  ...  autoFE_f_1831  autoFE_f_1832  autoFE_f_1833  \\\n",
       "id                          ...                                                \n",
       "0                       16  ...            0.0          -4.06            0.0   \n",
       "1                       16  ...            0.0          -4.06            0.0   \n",
       "2                       16  ...            0.0          -4.06            0.0   \n",
       "3                        2  ...            0.0          -4.06            0.0   \n",
       "4                       33  ...            0.0          -4.06            0.0   \n",
       "\n",
       "    autoFE_f_1834  autoFE_f_1835  autoFE_f_1836  autoFE_f_1837  autoFE_f_1838  \\\n",
       "id                                                                              \n",
       "0        0.476095       1.061701          190.0            0.0       5.676337   \n",
       "1        0.476095       0.872394          190.0            0.0       5.676337   \n",
       "2        0.497029       1.250124          190.0            0.0       5.087940   \n",
       "3        0.498465       0.872394          190.0            0.0       4.724530   \n",
       "4        0.497029       0.872394          190.0            0.0       5.087940   \n",
       "\n",
       "    autoFE_f_1839  autoFE_f_1840  \n",
       "id                                \n",
       "0        112006.0        58584.0  \n",
       "1        112006.0        27730.0  \n",
       "2        112006.0        27730.0  \n",
       "3        112006.0        58584.0  \n",
       "4        112006.0        58584.0  \n",
       "\n",
       "[5 rows x 1877 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1efd3149-3876-4186-b9a1-f11366470b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_description(col):\n",
    "    if not col.startswith('autoFE_f_'): return ''\n",
    "    idx = int(col.split('_')[-1])\n",
    "    return tree_to_formula(features[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf79d206-39d2-4f4c-85af-3b4910707378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(Curricular_units_2nd_sem_approved*Curricular_units_2nd_sem_grade)'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_description('autoFE_f_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4ee22b0-c2ff-45da-9328-32c96f726331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GroupByThenMin(Curricular_units_2nd_sem_enrolled,Nacionality)'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_description('autoFE_f_1831')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef9dc033-ba5e-49a7-b68d-44a2a3e640bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_train_df = pd.concat([train_x, train_y], axis=1)\n",
    "fe_test_df = test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5b05b99-8bf6-40db-a93c-bafad815332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_feats = [c for c in fe_train_df.columns if c != 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74b2392e-8652-4300-b6e6-7dd181e48f25",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=0 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.531524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150597\n",
      "[LightGBM] [Info] Number of data points in the train set: 61214, number of used features: 1870\n",
      "[LightGBM] [Info] Start training from score -0.746287\n",
      "[LightGBM] [Info] Start training from score -1.635907\n",
      "[LightGBM] [Info] Start training from score -1.105333\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.17185\n",
      "[400]\tvalid_0's multi_error: 0.167603\n",
      "[600]\tvalid_0's multi_error: 0.166492\n",
      "[800]\tvalid_0's multi_error: 0.166427\n",
      "[1000]\tvalid_0's multi_error: 0.165904\n",
      "Early stopping, best iteration is:\n",
      "[694]\tvalid_0's multi_error: 0.165578\n",
      "fold=1 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.622575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150474\n",
      "[LightGBM] [Info] Number of data points in the train set: 61214, number of used features: 1871\n",
      "[LightGBM] [Info] Start training from score -0.748012\n",
      "[LightGBM] [Info] Start training from score -1.626225\n",
      "[LightGBM] [Info] Start training from score -1.108594\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.167407\n",
      "[400]\tvalid_0's multi_error: 0.165186\n",
      "[600]\tvalid_0's multi_error: 0.163879\n",
      "[800]\tvalid_0's multi_error: 0.163291\n",
      "[1000]\tvalid_0's multi_error: 0.163225\n",
      "Early stopping, best iteration is:\n",
      "[690]\tvalid_0's multi_error: 0.162703\n",
      "fold=2 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.556287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150577\n",
      "[LightGBM] [Info] Number of data points in the train set: 61214, number of used features: 1870\n",
      "[LightGBM] [Info] Start training from score -0.746873\n",
      "[LightGBM] [Info] Start training from score -1.634231\n",
      "[LightGBM] [Info] Start training from score -1.105481\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.175183\n",
      "[400]\tvalid_0's multi_error: 0.171785\n",
      "[600]\tvalid_0's multi_error: 0.170021\n",
      "[800]\tvalid_0's multi_error: 0.169106\n",
      "[1000]\tvalid_0's multi_error: 0.169302\n",
      "[1200]\tvalid_0's multi_error: 0.169825\n",
      "Early stopping, best iteration is:\n",
      "[807]\tvalid_0's multi_error: 0.169041\n",
      "fold=3 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.593412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150780\n",
      "[LightGBM] [Info] Number of data points in the train set: 61215, number of used features: 1869\n",
      "[LightGBM] [Info] Start training from score -0.744273\n",
      "[LightGBM] [Info] Start training from score -1.631738\n",
      "[LightGBM] [Info] Start training from score -1.110692\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.176763\n",
      "[400]\tvalid_0's multi_error: 0.174672\n",
      "[600]\tvalid_0's multi_error: 0.173822\n",
      "[800]\tvalid_0's multi_error: 0.172188\n",
      "[1000]\tvalid_0's multi_error: 0.172188\n",
      "[1200]\tvalid_0's multi_error: 0.171731\n",
      "[1400]\tvalid_0's multi_error: 0.170882\n",
      "[1600]\tvalid_0's multi_error: 0.170293\n",
      "[1800]\tvalid_0's multi_error: 0.170816\n",
      "Early stopping, best iteration is:\n",
      "[1486]\tvalid_0's multi_error: 0.170163\n",
      "fold=4 - - - - - - - - - - - - - - - - - - - - \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.610226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150545\n",
      "[LightGBM] [Info] Number of data points in the train set: 61215, number of used features: 1869\n",
      "[LightGBM] [Info] Start training from score -0.745580\n",
      "[LightGBM] [Info] Start training from score -1.639368\n",
      "[LightGBM] [Info] Start training from score -1.104313\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's multi_error: 0.172973\n",
      "[400]\tvalid_0's multi_error: 0.170359\n",
      "[600]\tvalid_0's multi_error: 0.168529\n",
      "[800]\tvalid_0's multi_error: 0.167353\n",
      "[1000]\tvalid_0's multi_error: 0.16781\n",
      "[1200]\tvalid_0's multi_error: 0.167091\n",
      "[1400]\tvalid_0's multi_error: 0.167222\n",
      "[1600]\tvalid_0's multi_error: 0.166503\n",
      "[1800]\tvalid_0's multi_error: 0.166699\n",
      "[2000]\tvalid_0's multi_error: 0.166307\n",
      "[2200]\tvalid_0's multi_error: 0.166307\n",
      "[2400]\tvalid_0's multi_error: 0.166438\n",
      "[2600]\tvalid_0's multi_error: 0.166699\n",
      "Early stopping, best iteration is:\n",
      "[2315]\tvalid_0's multi_error: 0.165719\n"
     ]
    }
   ],
   "source": [
    "cv, oof, pred = lgb_cv(params, fe_train_df, fe_test_df, fe_feats, [], 'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9af9c0ec-cd77-4bba-ab5c-a93995797ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333594709741499\n"
     ]
    }
   ],
   "source": [
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88427cd-4969-41d8-a7f4-60fe7006fe28",
   "metadata": {},
   "source": [
    "#### 后向特征选择(通过autogluon实现)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4eaee7-526d-4df2-b9dd-3acba02f8635",
   "metadata": {},
   "source": [
    "AutoGluon的特征选择/修剪(feature_prune)功能在实验阶段，没有直接的API，这里的使用方法有些hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b8385-5002-4c58-98a2-76d8aa31f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里设置4h，迭代100轮，裁剪掉80%的特征，尽量少一点特征方便后续迭代优化\n",
    "hyperparameters = {'GBM':{}}\n",
    "feature_prune_kwargs = {'feature_prune_time_limit': 3600 * 4, 'n_train_subsample': None, 'n_fi_subsample': 20000, 'prune_ratio': 0.8, 'stopping_round': 100, 'min_improvement': 1e-6, 'force_prune': True}\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label='Target',\n",
    "    problem_type='multiclass',\n",
    "    eval_metric='accuracy',\n",
    ").fit(\n",
    "    fe_train_df,\n",
    "    hyperparameters=hyperparameters,\n",
    "    feature_prune_kwargs=feature_prune_kwargs,\n",
    "    presets='medium_quality',\n",
    "    time_limit=3600 * 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "33fb073f-8336-4b0e-a56b-650a22a8d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../automl/AutogluonModels/ag-20240619_064449/models/LightGBM_Prune/model.pkl', 'rb') as f:\n",
    "    prune_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d0e59ff-3fdc-4e14-bdcf-68d03694e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feats = prune_model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1e27b99-bf0a-4d23-8558-a82e64051f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d83a5b4-ca60-428e-8350-804c77819110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通常倾向于保留原始特征\n",
    "mix_selected_feats = ori_all_feats + [c for c in selected_feats if c not in ori_all_feats]\n",
    "len(mix_selected_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d0996-b70a-4b60-98af-74e36ad63708",
   "metadata": {},
   "source": [
    "## AutoGluon 模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9539c27a-0668-44cb-b346-3036a795c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5d3f07d7-b928-4985-9b14-af86bcc1c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_train_selected = read_pickle('fe_train_selected.pkl')\n",
    "fe_test_selected = read_pickle('fe_test_selected.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1a0046e0-bdaf-4013-9cd9-58cf18d20ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_imp_selected_cat_feats = read_pickle('null_imp_selected_cat_feats.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d839a7f5-338d-4040-b15a-3babd5a865d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in null_imp_selected_cat_feats:\n",
    "    fe_train_selected[col] = fe_train_selected[col].astype(int).astype('category')\n",
    "    fe_test_selected[col] = fe_test_selected[col].astype(int).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ec400ead-50c5-4bc0-a4db-cb3a9bcef242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76518, 56)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_train_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc292505-3d7f-497c-85df-dc79572b2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_predictor = TabularPredictor(\n",
    "    label='Target',\n",
    "    problem_type='multiclass',\n",
    "    eval_metric='accuracy',\n",
    "    verbosity=3,\n",
    ").fit(\n",
    "    fe_train_selected, \n",
    "    presets='best_quality', \n",
    "    num_bag_sets=1,\n",
    "    num_bag_folds=5,\n",
    "    num_stack_levels=3,\n",
    "    time_limit=3600 * 24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a5228772-f8e4-4d5d-b338-df5e1b5ee412",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_predictor = TabularPredictor.load('AutogluonModels/ag-20240626_021250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "300443a7-7142-4fd1-a158-f780468cd75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>0.835490</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>242.015740</td>\n",
       "      <td>15328.497464</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>5.314710</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost_r31_BAG_L3</td>\n",
       "      <td>0.835163</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>781.187671</td>\n",
       "      <td>31338.957801</td>\n",
       "      <td>3.507597</td>\n",
       "      <td>34.828593</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WeightedEnsemble_L5</td>\n",
       "      <td>0.835163</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>781.195905</td>\n",
       "      <td>31344.348478</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>5.390677</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L4</td>\n",
       "      <td>0.835163</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>781.196100</td>\n",
       "      <td>31344.334512</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>5.376711</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost_r95_BAG_L3</td>\n",
       "      <td>0.835111</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>781.269534</td>\n",
       "      <td>31332.827060</td>\n",
       "      <td>3.589459</td>\n",
       "      <td>28.697853</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>ExtraTrees_r42_BAG_L1</td>\n",
       "      <td>0.823989</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>4.926667</td>\n",
       "      <td>3.956279</td>\n",
       "      <td>4.926667</td>\n",
       "      <td>3.956279</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>NeuralNetTorch_r158_BAG_L1</td>\n",
       "      <td>0.823872</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.140451</td>\n",
       "      <td>264.488195</td>\n",
       "      <td>1.140451</td>\n",
       "      <td>264.488195</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>RandomForest_r16_BAG_L1</td>\n",
       "      <td>0.823074</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>4.902473</td>\n",
       "      <td>10.774498</td>\n",
       "      <td>4.902473</td>\n",
       "      <td>10.774498</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>KNeighborsUnif_BAG_L1</td>\n",
       "      <td>0.785136</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.979865</td>\n",
       "      <td>0.103050</td>\n",
       "      <td>1.979865</td>\n",
       "      <td>0.103050</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>KNeighborsDist_BAG_L1</td>\n",
       "      <td>0.780117</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>2.108359</td>\n",
       "      <td>0.087113</td>\n",
       "      <td>2.108359</td>\n",
       "      <td>0.087113</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  score_val eval_metric  pred_time_val  \\\n",
       "0           WeightedEnsemble_L3   0.835490    accuracy     242.015740   \n",
       "1            XGBoost_r31_BAG_L3   0.835163    accuracy     781.187671   \n",
       "2           WeightedEnsemble_L5   0.835163    accuracy     781.195905   \n",
       "3           WeightedEnsemble_L4   0.835163    accuracy     781.196100   \n",
       "4            XGBoost_r95_BAG_L3   0.835111    accuracy     781.269534   \n",
       "..                          ...        ...         ...            ...   \n",
       "433       ExtraTrees_r42_BAG_L1   0.823989    accuracy       4.926667   \n",
       "434  NeuralNetTorch_r158_BAG_L1   0.823872    accuracy       1.140451   \n",
       "435     RandomForest_r16_BAG_L1   0.823074    accuracy       4.902473   \n",
       "436       KNeighborsUnif_BAG_L1   0.785136    accuracy       1.979865   \n",
       "437       KNeighborsDist_BAG_L1   0.780117    accuracy       2.108359   \n",
       "\n",
       "         fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
       "0    15328.497464                0.008564           5.314710            3   \n",
       "1    31338.957801                3.507597          34.828593            3   \n",
       "2    31344.348478                0.008233           5.390677            5   \n",
       "3    31344.334512                0.008429           5.376711            4   \n",
       "4    31332.827060                3.589459          28.697853            3   \n",
       "..            ...                     ...                ...          ...   \n",
       "433      3.956279                4.926667           3.956279            1   \n",
       "434    264.488195                1.140451         264.488195            1   \n",
       "435     10.774498                4.902473          10.774498            1   \n",
       "436      0.103050                1.979865           0.103050            1   \n",
       "437      0.087113                2.108359           0.087113            1   \n",
       "\n",
       "     can_infer  fit_order  \n",
       "0         True        220  \n",
       "1         True        295  \n",
       "2         True        438  \n",
       "3         True        329  \n",
       "4         True        324  \n",
       "..         ...        ...  \n",
       "433       True         22  \n",
       "434       True         51  \n",
       "435       True         94  \n",
       "436       True          1  \n",
       "437       True          2  \n",
       "\n",
       "[438 rows x 10 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193629a-4a50-4230-9179-6715d2c4fe4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
